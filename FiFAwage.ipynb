{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Model Construction and Training\nThis notebook denotes the process of which the models are constructed and trained on the data. The following is the table of contents of this notebook. <br>\n\n--------\n1. Data Preprocessing\n2. Model Setup and Training <br>\n  2.1 Linear Regression and LASSO <br>\n  2.2 Random Forest <br>\n  2.3 Neural Network\n3. Model Evaluation\n4. Model Deployment\n5. Insights\n\n--------"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1 Data Preprocessing\nLoad the data before constructing the model. The goal of this section is to convert data to what a perfered type and deal with missing values. \n\n-------\nHere is what I have done in the data preprocessing stage: \n1. Convert the data into usable data <br>\n  1.1 Remove the players whose wage data is unavailable. Since the goal is to predict wage, adding necessary guesses at this stage is not preferred. <br> \n  1.2 Remove or replace unnecessary symbols such as \"\u20ac\", \"K\", etc. <br> \n  1.3 Transform data into integer type. For example, a height of a player could be 5'11''. It is converted to integer ($5\\times12+11 = 71$). <br>\n2. Replace missing values <br>\n  1.1 For some attributes that could not be zero (e.g. Height), the missing values are replaced with the median of the existing set <br>\n  1.2 For some attributes that could be zero, like players' scores (e.g. GKDiving, or gate keeper diving), the missing values are replaced by zero. <br>\n  \n-------\n"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200122180036-0000\nKERNEL_ID = b4148ef0-7ea8-4b63-a6fc-56819722d0b5\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_65d4a81f39cd4ebeba4f9b0b9d168ea8 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='TH6pdPiMrAco2paAMq7JAw5BIZIKjf59H5KdNRoO5F74',\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_65d4a81f39cd4ebeba4f9b0b9d168ea8.get_object(Bucket='couseraibmdatascienceproject-donotdelete-pr-fff0ca2ef4bcja',Key='data.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# If you are reading an Excel file into a pandas DataFrame, replace `read_csv` by `read_excel` in the next statement.\ndf = pd.read_csv(body)\ndf.head()", "execution_count": 2, "outputs": [{"output_type": "execute_result", "execution_count": 2, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>ID</th>\n      <th>Name</th>\n      <th>Age</th>\n      <th>Photo</th>\n      <th>Nationality</th>\n      <th>Flag</th>\n      <th>Overall</th>\n      <th>Potential</th>\n      <th>Club</th>\n      <th>...</th>\n      <th>Composure</th>\n      <th>Marking</th>\n      <th>StandingTackle</th>\n      <th>SlidingTackle</th>\n      <th>GKDiving</th>\n      <th>GKHandling</th>\n      <th>GKKicking</th>\n      <th>GKPositioning</th>\n      <th>GKReflexes</th>\n      <th>Release Clause</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>158023</td>\n      <td>L. Messi</td>\n      <td>31</td>\n      <td>https://cdn.sofifa.org/players/4/19/158023.png</td>\n      <td>Argentina</td>\n      <td>https://cdn.sofifa.org/flags/52.png</td>\n      <td>94</td>\n      <td>94</td>\n      <td>FC Barcelona</td>\n      <td>...</td>\n      <td>96.0</td>\n      <td>33.0</td>\n      <td>28.0</td>\n      <td>26.0</td>\n      <td>6.0</td>\n      <td>11.0</td>\n      <td>15.0</td>\n      <td>14.0</td>\n      <td>8.0</td>\n      <td>\u20ac226.5M</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20801</td>\n      <td>Cristiano Ronaldo</td>\n      <td>33</td>\n      <td>https://cdn.sofifa.org/players/4/19/20801.png</td>\n      <td>Portugal</td>\n      <td>https://cdn.sofifa.org/flags/38.png</td>\n      <td>94</td>\n      <td>94</td>\n      <td>Juventus</td>\n      <td>...</td>\n      <td>95.0</td>\n      <td>28.0</td>\n      <td>31.0</td>\n      <td>23.0</td>\n      <td>7.0</td>\n      <td>11.0</td>\n      <td>15.0</td>\n      <td>14.0</td>\n      <td>11.0</td>\n      <td>\u20ac127.1M</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>190871</td>\n      <td>Neymar Jr</td>\n      <td>26</td>\n      <td>https://cdn.sofifa.org/players/4/19/190871.png</td>\n      <td>Brazil</td>\n      <td>https://cdn.sofifa.org/flags/54.png</td>\n      <td>92</td>\n      <td>93</td>\n      <td>Paris Saint-Germain</td>\n      <td>...</td>\n      <td>94.0</td>\n      <td>27.0</td>\n      <td>24.0</td>\n      <td>33.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>11.0</td>\n      <td>\u20ac228.1M</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>193080</td>\n      <td>De Gea</td>\n      <td>27</td>\n      <td>https://cdn.sofifa.org/players/4/19/193080.png</td>\n      <td>Spain</td>\n      <td>https://cdn.sofifa.org/flags/45.png</td>\n      <td>91</td>\n      <td>93</td>\n      <td>Manchester United</td>\n      <td>...</td>\n      <td>68.0</td>\n      <td>15.0</td>\n      <td>21.0</td>\n      <td>13.0</td>\n      <td>90.0</td>\n      <td>85.0</td>\n      <td>87.0</td>\n      <td>88.0</td>\n      <td>94.0</td>\n      <td>\u20ac138.6M</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>192985</td>\n      <td>K. De Bruyne</td>\n      <td>27</td>\n      <td>https://cdn.sofifa.org/players/4/19/192985.png</td>\n      <td>Belgium</td>\n      <td>https://cdn.sofifa.org/flags/7.png</td>\n      <td>91</td>\n      <td>92</td>\n      <td>Manchester City</td>\n      <td>...</td>\n      <td>88.0</td>\n      <td>68.0</td>\n      <td>58.0</td>\n      <td>51.0</td>\n      <td>15.0</td>\n      <td>13.0</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>13.0</td>\n      <td>\u20ac196.4M</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 89 columns</p>\n</div>", "text/plain": "   Unnamed: 0      ID               Name  Age  \\\n0           0  158023           L. Messi   31   \n1           1   20801  Cristiano Ronaldo   33   \n2           2  190871          Neymar Jr   26   \n3           3  193080             De Gea   27   \n4           4  192985       K. De Bruyne   27   \n\n                                            Photo Nationality  \\\n0  https://cdn.sofifa.org/players/4/19/158023.png   Argentina   \n1   https://cdn.sofifa.org/players/4/19/20801.png    Portugal   \n2  https://cdn.sofifa.org/players/4/19/190871.png      Brazil   \n3  https://cdn.sofifa.org/players/4/19/193080.png       Spain   \n4  https://cdn.sofifa.org/players/4/19/192985.png     Belgium   \n\n                                  Flag  Overall  Potential  \\\n0  https://cdn.sofifa.org/flags/52.png       94         94   \n1  https://cdn.sofifa.org/flags/38.png       94         94   \n2  https://cdn.sofifa.org/flags/54.png       92         93   \n3  https://cdn.sofifa.org/flags/45.png       91         93   \n4   https://cdn.sofifa.org/flags/7.png       91         92   \n\n                  Club  ... Composure Marking StandingTackle  SlidingTackle  \\\n0         FC Barcelona  ...      96.0    33.0           28.0           26.0   \n1             Juventus  ...      95.0    28.0           31.0           23.0   \n2  Paris Saint-Germain  ...      94.0    27.0           24.0           33.0   \n3    Manchester United  ...      68.0    15.0           21.0           13.0   \n4      Manchester City  ...      88.0    68.0           58.0           51.0   \n\n  GKDiving  GKHandling  GKKicking  GKPositioning GKReflexes Release Clause  \n0      6.0        11.0       15.0           14.0        8.0        \u20ac226.5M  \n1      7.0        11.0       15.0           14.0       11.0        \u20ac127.1M  \n2      9.0         9.0       15.0           15.0       11.0        \u20ac228.1M  \n3     90.0        85.0       87.0           88.0       94.0        \u20ac138.6M  \n4     15.0        13.0        5.0           10.0       13.0        \u20ac196.4M  \n\n[5 rows x 89 columns]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# convert Wage from object to integer\ndf['Wage'] = df['Wage'].astype(str)\ndf['Wage'] = df['Wage'].str.replace('\u20ac', '')\ndf['Wage'] = df['Wage'].str.replace('K', '')\ndf['Wage'] = df['Wage'].astype(int)\ndf['Wage'].head()\ndf_1 = df[df.Wage != 0]", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# convert Value from object to int type\nvaluecol = df_1.columns.get_loc('Value')\ndf_1.loc[:, 'Value'] = df_1.loc[:, 'Value'].astype(str)\ndf_1['Value'] = df_1['Value'].str.replace('\u20ac', '')\n\nKs = np.where(df_1['Value'].str.contains('K'))[0]\nKs = np.ndarray.tolist(Ks)\ndf_1.iloc[Ks,valuecol] = df_1.iloc[Ks,valuecol].str.replace('K', '')\n\n                                                                                                                                   \nMs = np.where(df_1['Value'].str.contains('M'))[0]\nMs = np.ndarray.tolist(Ms)\ndf_1.iloc[Ms,valuecol] = df_1.iloc[Ms,valuecol].str.replace('M', '')\ndf_1.iloc[Ms,valuecol] = df_1.iloc[Ms,valuecol].astype(float) * 1000\n\ndf_1['Value'] = df_1['Value'].astype(int)", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# convert Height to int type\ndf_1copy = df_1.copy()\ndf_1copy['Height'] = df_1copy['Height'].astype(str)\nNAs = np.where(df_1copy['Height'] == 'nan')\nNAs = NAs[0]\ninches = df_1copy['Height'].str[0]\nfeet = df_1copy['Height'].str[2:]\ninches[inches == 'n'] = '0'\nfeet[feet == 'n'] = '0'\ndf_1copy['Height'] = inches.astype(int) * 12 + feet.astype(int)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_1['Height'] = df_1copy['Height']", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "hei = df_1.columns.get_loc('Height')\ndf_1.iloc[np.ndarray.tolist(NAs), hei] = df_1.Height[df_1.Height != 0].median()", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# convert weight to int type\ndf_1['Weight'] = df_1['Weight'].astype(str)\ndf_1['Weight'] = df_1['Weight'].str.replace('lbs','')\ndf_1.Weight[df_1.Weight == 'nan'] = '0'\ndf_1['Weight'] = df_1['Weight'].astype(int)\ndf_1.Weight[df_1.Weight == 0] = df_1.Weight[df_1.Weight != 0].median()", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "records = ['Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', \n         'LongPassing', 'BallControl', 'Acceleration','SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', \n         'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', \n         'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', \n         'GKPositioning', 'GKReflexes']\ndf_1[records] = df_1[records].fillna(0)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "allcol = ['Wage', 'Age', 'Overall', 'Potential', 'Value', 'Height', 'Weight', 'Crossing', 'Finishing', \n              'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', \n              'BallControl', 'Acceleration','SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', \n              'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', \n              'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', \n              'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes' ]\ndf_1[allcol].head()", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wage</th>\n      <th>Age</th>\n      <th>Overall</th>\n      <th>Potential</th>\n      <th>Value</th>\n      <th>Height</th>\n      <th>Weight</th>\n      <th>Crossing</th>\n      <th>Finishing</th>\n      <th>HeadingAccuracy</th>\n      <th>...</th>\n      <th>Penalties</th>\n      <th>Composure</th>\n      <th>Marking</th>\n      <th>StandingTackle</th>\n      <th>SlidingTackle</th>\n      <th>GKDiving</th>\n      <th>GKHandling</th>\n      <th>GKKicking</th>\n      <th>GKPositioning</th>\n      <th>GKReflexes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>565</td>\n      <td>31</td>\n      <td>94</td>\n      <td>94</td>\n      <td>110500</td>\n      <td>67.0</td>\n      <td>159</td>\n      <td>84.0</td>\n      <td>95.0</td>\n      <td>70.0</td>\n      <td>...</td>\n      <td>75.0</td>\n      <td>96.0</td>\n      <td>33.0</td>\n      <td>28.0</td>\n      <td>26.0</td>\n      <td>6.0</td>\n      <td>11.0</td>\n      <td>15.0</td>\n      <td>14.0</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>405</td>\n      <td>33</td>\n      <td>94</td>\n      <td>94</td>\n      <td>77000</td>\n      <td>74.0</td>\n      <td>183</td>\n      <td>84.0</td>\n      <td>94.0</td>\n      <td>89.0</td>\n      <td>...</td>\n      <td>85.0</td>\n      <td>95.0</td>\n      <td>28.0</td>\n      <td>31.0</td>\n      <td>23.0</td>\n      <td>7.0</td>\n      <td>11.0</td>\n      <td>15.0</td>\n      <td>14.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>290</td>\n      <td>26</td>\n      <td>92</td>\n      <td>93</td>\n      <td>118500</td>\n      <td>69.0</td>\n      <td>150</td>\n      <td>79.0</td>\n      <td>87.0</td>\n      <td>62.0</td>\n      <td>...</td>\n      <td>81.0</td>\n      <td>94.0</td>\n      <td>27.0</td>\n      <td>24.0</td>\n      <td>33.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>260</td>\n      <td>27</td>\n      <td>91</td>\n      <td>93</td>\n      <td>72000</td>\n      <td>76.0</td>\n      <td>168</td>\n      <td>17.0</td>\n      <td>13.0</td>\n      <td>21.0</td>\n      <td>...</td>\n      <td>40.0</td>\n      <td>68.0</td>\n      <td>15.0</td>\n      <td>21.0</td>\n      <td>13.0</td>\n      <td>90.0</td>\n      <td>85.0</td>\n      <td>87.0</td>\n      <td>88.0</td>\n      <td>94.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>355</td>\n      <td>27</td>\n      <td>91</td>\n      <td>92</td>\n      <td>102000</td>\n      <td>71.0</td>\n      <td>154</td>\n      <td>93.0</td>\n      <td>82.0</td>\n      <td>55.0</td>\n      <td>...</td>\n      <td>79.0</td>\n      <td>88.0</td>\n      <td>68.0</td>\n      <td>58.0</td>\n      <td>51.0</td>\n      <td>15.0</td>\n      <td>13.0</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>13.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 41 columns</p>\n</div>", "text/plain": "   Wage  Age  Overall  Potential   Value  Height  Weight  Crossing  Finishing  \\\n0   565   31       94         94  110500    67.0     159      84.0       95.0   \n1   405   33       94         94   77000    74.0     183      84.0       94.0   \n2   290   26       92         93  118500    69.0     150      79.0       87.0   \n3   260   27       91         93   72000    76.0     168      17.0       13.0   \n4   355   27       91         92  102000    71.0     154      93.0       82.0   \n\n   HeadingAccuracy  ...  Penalties  Composure  Marking  StandingTackle  \\\n0             70.0  ...       75.0       96.0     33.0            28.0   \n1             89.0  ...       85.0       95.0     28.0            31.0   \n2             62.0  ...       81.0       94.0     27.0            24.0   \n3             21.0  ...       40.0       68.0     15.0            21.0   \n4             55.0  ...       79.0       88.0     68.0            58.0   \n\n   SlidingTackle  GKDiving  GKHandling  GKKicking  GKPositioning  GKReflexes  \n0           26.0       6.0        11.0       15.0           14.0         8.0  \n1           23.0       7.0        11.0       15.0           14.0        11.0  \n2           33.0       9.0         9.0       15.0           15.0        11.0  \n3           13.0      90.0        85.0       87.0           88.0        94.0  \n4           51.0      15.0        13.0        5.0           10.0        13.0  \n\n[5 rows x 41 columns]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "pd.set_option('display.max_columns', None)", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pd.DataFrame.describe(df_1[allcol])", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wage</th>\n      <th>Age</th>\n      <th>Overall</th>\n      <th>Potential</th>\n      <th>Value</th>\n      <th>Height</th>\n      <th>Weight</th>\n      <th>Crossing</th>\n      <th>Finishing</th>\n      <th>HeadingAccuracy</th>\n      <th>ShortPassing</th>\n      <th>Volleys</th>\n      <th>Dribbling</th>\n      <th>Curve</th>\n      <th>FKAccuracy</th>\n      <th>LongPassing</th>\n      <th>BallControl</th>\n      <th>Acceleration</th>\n      <th>SprintSpeed</th>\n      <th>Agility</th>\n      <th>Reactions</th>\n      <th>Balance</th>\n      <th>ShotPower</th>\n      <th>Jumping</th>\n      <th>Stamina</th>\n      <th>Strength</th>\n      <th>LongShots</th>\n      <th>Aggression</th>\n      <th>Interceptions</th>\n      <th>Positioning</th>\n      <th>Vision</th>\n      <th>Penalties</th>\n      <th>Composure</th>\n      <th>Marking</th>\n      <th>StandingTackle</th>\n      <th>SlidingTackle</th>\n      <th>GKDiving</th>\n      <th>GKHandling</th>\n      <th>GKKicking</th>\n      <th>GKPositioning</th>\n      <th>GKReflexes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n      <td>17966.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>9.861850</td>\n      <td>25.104976</td>\n      <td>66.225481</td>\n      <td>71.317322</td>\n      <td>2443.033508</td>\n      <td>71.360069</td>\n      <td>165.966270</td>\n      <td>49.615941</td>\n      <td>45.459368</td>\n      <td>52.155572</td>\n      <td>58.556551</td>\n      <td>42.817433</td>\n      <td>55.263164</td>\n      <td>47.089669</td>\n      <td>42.767450</td>\n      <td>52.583435</td>\n      <td>58.255093</td>\n      <td>64.428921</td>\n      <td>64.547757</td>\n      <td>63.353668</td>\n      <td>61.655349</td>\n      <td>63.791495</td>\n      <td>55.342258</td>\n      <td>64.942503</td>\n      <td>63.037126</td>\n      <td>65.149171</td>\n      <td>47.004397</td>\n      <td>55.729767</td>\n      <td>46.566125</td>\n      <td>49.862184</td>\n      <td>53.306134</td>\n      <td>48.414783</td>\n      <td>58.498553</td>\n      <td>47.137816</td>\n      <td>47.556607</td>\n      <td>45.521262</td>\n      <td>16.546031</td>\n      <td>16.323945</td>\n      <td>16.162362</td>\n      <td>16.316598</td>\n      <td>16.639096</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>22.117274</td>\n      <td>4.674724</td>\n      <td>6.923435</td>\n      <td>6.146192</td>\n      <td>5625.317578</td>\n      <td>2.646264</td>\n      <td>15.583305</td>\n      <td>18.509471</td>\n      <td>19.627990</td>\n      <td>17.553427</td>\n      <td>14.970724</td>\n      <td>17.803028</td>\n      <td>19.094622</td>\n      <td>18.525890</td>\n      <td>17.599535</td>\n      <td>15.531239</td>\n      <td>16.914217</td>\n      <td>15.295182</td>\n      <td>15.022704</td>\n      <td>15.108814</td>\n      <td>9.560669</td>\n      <td>14.511473</td>\n      <td>17.429900</td>\n      <td>12.289756</td>\n      <td>16.203545</td>\n      <td>12.981089</td>\n      <td>19.379105</td>\n      <td>17.569552</td>\n      <td>20.804267</td>\n      <td>19.665096</td>\n      <td>14.367725</td>\n      <td>15.869684</td>\n      <td>11.800752</td>\n      <td>19.997571</td>\n      <td>21.758416</td>\n      <td>21.372569</td>\n      <td>17.658866</td>\n      <td>16.876372</td>\n      <td>16.467775</td>\n      <td>16.990492</td>\n      <td>17.912895</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>16.000000</td>\n      <td>46.000000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n      <td>61.000000</td>\n      <td>110.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>21.000000</td>\n      <td>62.000000</td>\n      <td>67.000000</td>\n      <td>325.000000</td>\n      <td>69.000000</td>\n      <td>154.000000</td>\n      <td>38.000000</td>\n      <td>30.000000</td>\n      <td>44.000000</td>\n      <td>53.000000</td>\n      <td>30.000000</td>\n      <td>49.000000</td>\n      <td>34.000000</td>\n      <td>31.000000</td>\n      <td>43.000000</td>\n      <td>54.000000</td>\n      <td>57.000000</td>\n      <td>57.000000</td>\n      <td>55.000000</td>\n      <td>56.000000</td>\n      <td>56.000000</td>\n      <td>45.000000</td>\n      <td>58.000000</td>\n      <td>56.000000</td>\n      <td>58.000000</td>\n      <td>33.000000</td>\n      <td>44.000000</td>\n      <td>26.000000</td>\n      <td>38.000000</td>\n      <td>44.000000</td>\n      <td>39.000000</td>\n      <td>51.000000</td>\n      <td>30.000000</td>\n      <td>26.000000</td>\n      <td>24.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>25.000000</td>\n      <td>66.000000</td>\n      <td>71.000000</td>\n      <td>700.000000</td>\n      <td>71.000000</td>\n      <td>165.000000</td>\n      <td>54.000000</td>\n      <td>49.000000</td>\n      <td>56.000000</td>\n      <td>62.000000</td>\n      <td>44.000000</td>\n      <td>61.000000</td>\n      <td>48.000000</td>\n      <td>41.000000</td>\n      <td>56.000000</td>\n      <td>63.000000</td>\n      <td>67.000000</td>\n      <td>67.000000</td>\n      <td>66.000000</td>\n      <td>62.000000</td>\n      <td>66.000000</td>\n      <td>59.000000</td>\n      <td>66.000000</td>\n      <td>66.000000</td>\n      <td>66.000000</td>\n      <td>51.000000</td>\n      <td>59.000000</td>\n      <td>52.000000</td>\n      <td>55.000000</td>\n      <td>55.000000</td>\n      <td>49.000000</td>\n      <td>59.000000</td>\n      <td>52.000000</td>\n      <td>55.000000</td>\n      <td>52.000000</td>\n      <td>11.000000</td>\n      <td>11.000000</td>\n      <td>11.000000</td>\n      <td>11.000000</td>\n      <td>11.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.000000</td>\n      <td>28.000000</td>\n      <td>71.000000</td>\n      <td>75.000000</td>\n      <td>2000.000000</td>\n      <td>73.000000</td>\n      <td>176.000000</td>\n      <td>64.000000</td>\n      <td>62.000000</td>\n      <td>64.000000</td>\n      <td>68.000000</td>\n      <td>57.000000</td>\n      <td>68.000000</td>\n      <td>62.000000</td>\n      <td>56.000000</td>\n      <td>64.000000</td>\n      <td>69.000000</td>\n      <td>75.000000</td>\n      <td>75.000000</td>\n      <td>74.000000</td>\n      <td>68.000000</td>\n      <td>74.000000</td>\n      <td>68.000000</td>\n      <td>73.000000</td>\n      <td>74.000000</td>\n      <td>74.000000</td>\n      <td>62.000000</td>\n      <td>69.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>60.000000</td>\n      <td>67.000000</td>\n      <td>64.000000</td>\n      <td>66.000000</td>\n      <td>64.000000</td>\n      <td>14.000000</td>\n      <td>14.000000</td>\n      <td>14.000000</td>\n      <td>14.000000</td>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>565.000000</td>\n      <td>45.000000</td>\n      <td>94.000000</td>\n      <td>95.000000</td>\n      <td>118500.000000</td>\n      <td>81.000000</td>\n      <td>243.000000</td>\n      <td>93.000000</td>\n      <td>95.000000</td>\n      <td>94.000000</td>\n      <td>93.000000</td>\n      <td>90.000000</td>\n      <td>97.000000</td>\n      <td>94.000000</td>\n      <td>94.000000</td>\n      <td>93.000000</td>\n      <td>96.000000</td>\n      <td>97.000000</td>\n      <td>96.000000</td>\n      <td>96.000000</td>\n      <td>96.000000</td>\n      <td>96.000000</td>\n      <td>95.000000</td>\n      <td>95.000000</td>\n      <td>96.000000</td>\n      <td>97.000000</td>\n      <td>94.000000</td>\n      <td>95.000000</td>\n      <td>92.000000</td>\n      <td>95.000000</td>\n      <td>94.000000</td>\n      <td>92.000000</td>\n      <td>96.000000</td>\n      <td>94.000000</td>\n      <td>93.000000</td>\n      <td>91.000000</td>\n      <td>90.000000</td>\n      <td>92.000000</td>\n      <td>91.000000</td>\n      <td>90.000000</td>\n      <td>94.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "               Wage           Age       Overall     Potential          Value  \\\ncount  17966.000000  17966.000000  17966.000000  17966.000000   17966.000000   \nmean       9.861850     25.104976     66.225481     71.317322    2443.033508   \nstd       22.117274      4.674724      6.923435      6.146192    5625.317578   \nmin        1.000000     16.000000     46.000000     48.000000       0.000000   \n25%        1.000000     21.000000     62.000000     67.000000     325.000000   \n50%        3.000000     25.000000     66.000000     71.000000     700.000000   \n75%        9.000000     28.000000     71.000000     75.000000    2000.000000   \nmax      565.000000     45.000000     94.000000     95.000000  118500.000000   \n\n             Height        Weight      Crossing     Finishing  \\\ncount  17966.000000  17966.000000  17966.000000  17966.000000   \nmean      71.360069    165.966270     49.615941     45.459368   \nstd        2.646264     15.583305     18.509471     19.627990   \nmin       61.000000    110.000000      0.000000      0.000000   \n25%       69.000000    154.000000     38.000000     30.000000   \n50%       71.000000    165.000000     54.000000     49.000000   \n75%       73.000000    176.000000     64.000000     62.000000   \nmax       81.000000    243.000000     93.000000     95.000000   \n\n       HeadingAccuracy  ShortPassing       Volleys     Dribbling  \\\ncount     17966.000000  17966.000000  17966.000000  17966.000000   \nmean         52.155572     58.556551     42.817433     55.263164   \nstd          17.553427     14.970724     17.803028     19.094622   \nmin           0.000000      0.000000      0.000000      0.000000   \n25%          44.000000     53.000000     30.000000     49.000000   \n50%          56.000000     62.000000     44.000000     61.000000   \n75%          64.000000     68.000000     57.000000     68.000000   \nmax          94.000000     93.000000     90.000000     97.000000   \n\n              Curve    FKAccuracy   LongPassing   BallControl  Acceleration  \\\ncount  17966.000000  17966.000000  17966.000000  17966.000000  17966.000000   \nmean      47.089669     42.767450     52.583435     58.255093     64.428921   \nstd       18.525890     17.599535     15.531239     16.914217     15.295182   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%       34.000000     31.000000     43.000000     54.000000     57.000000   \n50%       48.000000     41.000000     56.000000     63.000000     67.000000   \n75%       62.000000     56.000000     64.000000     69.000000     75.000000   \nmax       94.000000     94.000000     93.000000     96.000000     97.000000   \n\n        SprintSpeed       Agility     Reactions       Balance     ShotPower  \\\ncount  17966.000000  17966.000000  17966.000000  17966.000000  17966.000000   \nmean      64.547757     63.353668     61.655349     63.791495     55.342258   \nstd       15.022704     15.108814      9.560669     14.511473     17.429900   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%       57.000000     55.000000     56.000000     56.000000     45.000000   \n50%       67.000000     66.000000     62.000000     66.000000     59.000000   \n75%       75.000000     74.000000     68.000000     74.000000     68.000000   \nmax       96.000000     96.000000     96.000000     96.000000     95.000000   \n\n            Jumping       Stamina      Strength     LongShots    Aggression  \\\ncount  17966.000000  17966.000000  17966.000000  17966.000000  17966.000000   \nmean      64.942503     63.037126     65.149171     47.004397     55.729767   \nstd       12.289756     16.203545     12.981089     19.379105     17.569552   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%       58.000000     56.000000     58.000000     33.000000     44.000000   \n50%       66.000000     66.000000     66.000000     51.000000     59.000000   \n75%       73.000000     74.000000     74.000000     62.000000     69.000000   \nmax       95.000000     96.000000     97.000000     94.000000     95.000000   \n\n       Interceptions   Positioning        Vision     Penalties     Composure  \\\ncount   17966.000000  17966.000000  17966.000000  17966.000000  17966.000000   \nmean       46.566125     49.862184     53.306134     48.414783     58.498553   \nstd        20.804267     19.665096     14.367725     15.869684     11.800752   \nmin         0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        26.000000     38.000000     44.000000     39.000000     51.000000   \n50%        52.000000     55.000000     55.000000     49.000000     59.000000   \n75%        64.000000     64.000000     64.000000     60.000000     67.000000   \nmax        92.000000     95.000000     94.000000     92.000000     96.000000   \n\n            Marking  StandingTackle  SlidingTackle      GKDiving  \\\ncount  17966.000000    17966.000000   17966.000000  17966.000000   \nmean      47.137816       47.556607      45.521262     16.546031   \nstd       19.997571       21.758416      21.372569     17.658866   \nmin        0.000000        0.000000       0.000000      0.000000   \n25%       30.000000       26.000000      24.000000      8.000000   \n50%       52.000000       55.000000      52.000000     11.000000   \n75%       64.000000       66.000000      64.000000     14.000000   \nmax       94.000000       93.000000      91.000000     90.000000   \n\n         GKHandling     GKKicking  GKPositioning    GKReflexes  \ncount  17966.000000  17966.000000   17966.000000  17966.000000  \nmean      16.323945     16.162362      16.316598     16.639096  \nstd       16.876372     16.467775      16.990492     17.912895  \nmin        0.000000      0.000000       0.000000      0.000000  \n25%        8.000000      8.000000       8.000000      8.000000  \n50%       11.000000     11.000000      11.000000     11.000000  \n75%       14.000000     14.000000      14.000000     14.000000  \nmax       92.000000     91.000000      90.000000     94.000000  "}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "pd.DataFrame.corr(df_1[allcol])", "execution_count": 13, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wage</th>\n      <th>Age</th>\n      <th>Overall</th>\n      <th>Potential</th>\n      <th>Value</th>\n      <th>Height</th>\n      <th>Weight</th>\n      <th>Crossing</th>\n      <th>Finishing</th>\n      <th>HeadingAccuracy</th>\n      <th>ShortPassing</th>\n      <th>Volleys</th>\n      <th>Dribbling</th>\n      <th>Curve</th>\n      <th>FKAccuracy</th>\n      <th>LongPassing</th>\n      <th>BallControl</th>\n      <th>Acceleration</th>\n      <th>SprintSpeed</th>\n      <th>Agility</th>\n      <th>Reactions</th>\n      <th>Balance</th>\n      <th>ShotPower</th>\n      <th>Jumping</th>\n      <th>Stamina</th>\n      <th>Strength</th>\n      <th>LongShots</th>\n      <th>Aggression</th>\n      <th>Interceptions</th>\n      <th>Positioning</th>\n      <th>Vision</th>\n      <th>Penalties</th>\n      <th>Composure</th>\n      <th>Marking</th>\n      <th>StandingTackle</th>\n      <th>SlidingTackle</th>\n      <th>GKDiving</th>\n      <th>GKHandling</th>\n      <th>GKKicking</th>\n      <th>GKPositioning</th>\n      <th>GKReflexes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Wage</th>\n      <td>1.000000</td>\n      <td>0.143773</td>\n      <td>0.576150</td>\n      <td>0.488808</td>\n      <td>0.858080</td>\n      <td>0.019966</td>\n      <td>0.065594</td>\n      <td>0.234813</td>\n      <td>0.219319</td>\n      <td>0.190332</td>\n      <td>0.296287</td>\n      <td>0.259252</td>\n      <td>0.238315</td>\n      <td>0.260829</td>\n      <td>0.238217</td>\n      <td>0.277910</td>\n      <td>0.277971</td>\n      <td>0.127312</td>\n      <td>0.132349</td>\n      <td>0.157302</td>\n      <td>0.477211</td>\n      <td>0.091619</td>\n      <td>0.259531</td>\n      <td>0.129796</td>\n      <td>0.179546</td>\n      <td>0.140250</td>\n      <td>0.251102</td>\n      <td>0.196484</td>\n      <td>0.159995</td>\n      <td>0.228335</td>\n      <td>0.314749</td>\n      <td>0.224685</td>\n      <td>0.413919</td>\n      <td>0.148582</td>\n      <td>0.129007</td>\n      <td>0.113800</td>\n      <td>-0.024234</td>\n      <td>-0.023797</td>\n      <td>-0.026884</td>\n      <td>-0.024020</td>\n      <td>-0.024625</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>0.143773</td>\n      <td>1.000000</td>\n      <td>0.452519</td>\n      <td>-0.253846</td>\n      <td>0.077141</td>\n      <td>0.081980</td>\n      <td>0.229551</td>\n      <td>0.130518</td>\n      <td>0.069622</td>\n      <td>0.147051</td>\n      <td>0.132018</td>\n      <td>0.142423</td>\n      <td>0.011541</td>\n      <td>0.143599</td>\n      <td>0.193484</td>\n      <td>0.180289</td>\n      <td>0.085402</td>\n      <td>-0.154191</td>\n      <td>-0.146779</td>\n      <td>-0.017642</td>\n      <td>0.427635</td>\n      <td>-0.086890</td>\n      <td>0.156358</td>\n      <td>0.172238</td>\n      <td>0.097799</td>\n      <td>0.323395</td>\n      <td>0.155028</td>\n      <td>0.263356</td>\n      <td>0.197649</td>\n      <td>0.083854</td>\n      <td>0.185797</td>\n      <td>0.138605</td>\n      <td>0.379930</td>\n      <td>0.143118</td>\n      <td>0.119987</td>\n      <td>0.103082</td>\n      <td>0.099971</td>\n      <td>0.105088</td>\n      <td>0.103533</td>\n      <td>0.115344</td>\n      <td>0.102066</td>\n    </tr>\n    <tr>\n      <th>Overall</th>\n      <td>0.576150</td>\n      <td>0.452519</td>\n      <td>1.000000</td>\n      <td>0.660605</td>\n      <td>0.631570</td>\n      <td>0.038444</td>\n      <td>0.154119</td>\n      <td>0.396615</td>\n      <td>0.335101</td>\n      <td>0.342988</td>\n      <td>0.499904</td>\n      <td>0.392951</td>\n      <td>0.374591</td>\n      <td>0.420967</td>\n      <td>0.398798</td>\n      <td>0.482774</td>\n      <td>0.460365</td>\n      <td>0.199887</td>\n      <td>0.213534</td>\n      <td>0.266740</td>\n      <td>0.812473</td>\n      <td>0.108357</td>\n      <td>0.442167</td>\n      <td>0.263994</td>\n      <td>0.365773</td>\n      <td>0.347049</td>\n      <td>0.422599</td>\n      <td>0.396327</td>\n      <td>0.324114</td>\n      <td>0.359001</td>\n      <td>0.497630</td>\n      <td>0.342878</td>\n      <td>0.713420</td>\n      <td>0.289565</td>\n      <td>0.255716</td>\n      <td>0.225634</td>\n      <td>-0.024982</td>\n      <td>-0.024332</td>\n      <td>-0.028483</td>\n      <td>-0.016577</td>\n      <td>-0.022430</td>\n    </tr>\n    <tr>\n      <th>Potential</th>\n      <td>0.488808</td>\n      <td>-0.253846</td>\n      <td>0.660605</td>\n      <td>1.000000</td>\n      <td>0.579170</td>\n      <td>-0.009818</td>\n      <td>-0.007530</td>\n      <td>0.249085</td>\n      <td>0.245919</td>\n      <td>0.204485</td>\n      <td>0.368916</td>\n      <td>0.257102</td>\n      <td>0.317479</td>\n      <td>0.281499</td>\n      <td>0.232954</td>\n      <td>0.322378</td>\n      <td>0.355717</td>\n      <td>0.238047</td>\n      <td>0.239984</td>\n      <td>0.225557</td>\n      <td>0.496197</td>\n      <td>0.143630</td>\n      <td>0.290572</td>\n      <td>0.114496</td>\n      <td>0.205499</td>\n      <td>0.082126</td>\n      <td>0.268951</td>\n      <td>0.173991</td>\n      <td>0.158222</td>\n      <td>0.248104</td>\n      <td>0.348328</td>\n      <td>0.227321</td>\n      <td>0.434466</td>\n      <td>0.166234</td>\n      <td>0.147085</td>\n      <td>0.132381</td>\n      <td>-0.051163</td>\n      <td>-0.052652</td>\n      <td>-0.056843</td>\n      <td>-0.050387</td>\n      <td>-0.051285</td>\n    </tr>\n    <tr>\n      <th>Value</th>\n      <td>0.858080</td>\n      <td>0.077141</td>\n      <td>0.631570</td>\n      <td>0.579170</td>\n      <td>1.000000</td>\n      <td>0.002879</td>\n      <td>0.046616</td>\n      <td>0.251745</td>\n      <td>0.258606</td>\n      <td>0.186815</td>\n      <td>0.326873</td>\n      <td>0.290156</td>\n      <td>0.273076</td>\n      <td>0.288518</td>\n      <td>0.267599</td>\n      <td>0.303158</td>\n      <td>0.308977</td>\n      <td>0.172122</td>\n      <td>0.173931</td>\n      <td>0.194676</td>\n      <td>0.519866</td>\n      <td>0.115995</td>\n      <td>0.282451</td>\n      <td>0.124827</td>\n      <td>0.212278</td>\n      <td>0.130114</td>\n      <td>0.281763</td>\n      <td>0.186601</td>\n      <td>0.143222</td>\n      <td>0.260952</td>\n      <td>0.356646</td>\n      <td>0.241207</td>\n      <td>0.443918</td>\n      <td>0.136821</td>\n      <td>0.111076</td>\n      <td>0.090444</td>\n      <td>-0.027272</td>\n      <td>-0.027555</td>\n      <td>-0.029515</td>\n      <td>-0.026458</td>\n      <td>-0.027211</td>\n    </tr>\n    <tr>\n      <th>Height</th>\n      <td>0.019966</td>\n      <td>0.081980</td>\n      <td>0.038444</td>\n      <td>-0.009818</td>\n      <td>0.002879</td>\n      <td>1.000000</td>\n      <td>0.754278</td>\n      <td>-0.481792</td>\n      <td>-0.366690</td>\n      <td>0.013868</td>\n      <td>-0.357552</td>\n      <td>-0.345871</td>\n      <td>-0.486929</td>\n      <td>-0.436651</td>\n      <td>-0.400265</td>\n      <td>-0.325218</td>\n      <td>-0.408868</td>\n      <td>-0.531553</td>\n      <td>-0.452834</td>\n      <td>-0.606220</td>\n      <td>-0.017717</td>\n      <td>-0.762855</td>\n      <td>-0.286300</td>\n      <td>-0.065480</td>\n      <td>-0.280670</td>\n      <td>0.519396</td>\n      <td>-0.377344</td>\n      <td>-0.042109</td>\n      <td>-0.049344</td>\n      <td>-0.430577</td>\n      <td>-0.360994</td>\n      <td>-0.335078</td>\n      <td>-0.129665</td>\n      <td>-0.072609</td>\n      <td>-0.057870</td>\n      <td>-0.066151</td>\n      <td>0.360583</td>\n      <td>0.360914</td>\n      <td>0.358906</td>\n      <td>0.362008</td>\n      <td>0.362594</td>\n    </tr>\n    <tr>\n      <th>Weight</th>\n      <td>0.065594</td>\n      <td>0.229551</td>\n      <td>0.154119</td>\n      <td>-0.007530</td>\n      <td>0.046616</td>\n      <td>0.754278</td>\n      <td>1.000000</td>\n      <td>-0.389106</td>\n      <td>-0.289526</td>\n      <td>0.037412</td>\n      <td>-0.283021</td>\n      <td>-0.260315</td>\n      <td>-0.408437</td>\n      <td>-0.342680</td>\n      <td>-0.301647</td>\n      <td>-0.255799</td>\n      <td>-0.331395</td>\n      <td>-0.465779</td>\n      <td>-0.399840</td>\n      <td>-0.521236</td>\n      <td>0.082039</td>\n      <td>-0.645334</td>\n      <td>-0.187543</td>\n      <td>0.009483</td>\n      <td>-0.216783</td>\n      <td>0.596124</td>\n      <td>-0.275134</td>\n      <td>0.031942</td>\n      <td>-0.024375</td>\n      <td>-0.346132</td>\n      <td>-0.277647</td>\n      <td>-0.249198</td>\n      <td>-0.032196</td>\n      <td>-0.047815</td>\n      <td>-0.045426</td>\n      <td>-0.054921</td>\n      <td>0.338844</td>\n      <td>0.338018</td>\n      <td>0.336873</td>\n      <td>0.341163</td>\n      <td>0.340019</td>\n    </tr>\n    <tr>\n      <th>Crossing</th>\n      <td>0.234813</td>\n      <td>0.130518</td>\n      <td>0.396615</td>\n      <td>0.249085</td>\n      <td>0.251745</td>\n      <td>-0.481792</td>\n      <td>-0.389106</td>\n      <td>1.000000</td>\n      <td>0.661233</td>\n      <td>0.479991</td>\n      <td>0.813317</td>\n      <td>0.695608</td>\n      <td>0.859764</td>\n      <td>0.837136</td>\n      <td>0.765394</td>\n      <td>0.762133</td>\n      <td>0.844315</td>\n      <td>0.675733</td>\n      <td>0.653687</td>\n      <td>0.704819</td>\n      <td>0.410975</td>\n      <td>0.627755</td>\n      <td>0.712821</td>\n      <td>0.167838</td>\n      <td>0.679565</td>\n      <td>0.007803</td>\n      <td>0.746821</td>\n      <td>0.485786</td>\n      <td>0.436860</td>\n      <td>0.787018</td>\n      <td>0.692419</td>\n      <td>0.653712</td>\n      <td>0.587027</td>\n      <td>0.452757</td>\n      <td>0.437817</td>\n      <td>0.418693</td>\n      <td>-0.648413</td>\n      <td>-0.645549</td>\n      <td>-0.644819</td>\n      <td>-0.645475</td>\n      <td>-0.648011</td>\n    </tr>\n    <tr>\n      <th>Finishing</th>\n      <td>0.219319</td>\n      <td>0.069622</td>\n      <td>0.335101</td>\n      <td>0.245919</td>\n      <td>0.258606</td>\n      <td>-0.366690</td>\n      <td>-0.289526</td>\n      <td>0.661233</td>\n      <td>1.000000</td>\n      <td>0.483089</td>\n      <td>0.667675</td>\n      <td>0.884473</td>\n      <td>0.826993</td>\n      <td>0.762459</td>\n      <td>0.701886</td>\n      <td>0.522642</td>\n      <td>0.791308</td>\n      <td>0.613557</td>\n      <td>0.601499</td>\n      <td>0.650450</td>\n      <td>0.351554</td>\n      <td>0.533800</td>\n      <td>0.818319</td>\n      <td>0.126284</td>\n      <td>0.520971</td>\n      <td>0.022010</td>\n      <td>0.879485</td>\n      <td>0.259058</td>\n      <td>-0.006059</td>\n      <td>0.890732</td>\n      <td>0.702071</td>\n      <td>0.840845</td>\n      <td>0.543090</td>\n      <td>0.039418</td>\n      <td>-0.018439</td>\n      <td>-0.057634</td>\n      <td>-0.577767</td>\n      <td>-0.576122</td>\n      <td>-0.572268</td>\n      <td>-0.573927</td>\n      <td>-0.576336</td>\n    </tr>\n    <tr>\n      <th>HeadingAccuracy</th>\n      <td>0.190332</td>\n      <td>0.147051</td>\n      <td>0.342988</td>\n      <td>0.204485</td>\n      <td>0.186815</td>\n      <td>0.013868</td>\n      <td>0.037412</td>\n      <td>0.479991</td>\n      <td>0.483089</td>\n      <td>1.000000</td>\n      <td>0.650725</td>\n      <td>0.515015</td>\n      <td>0.561127</td>\n      <td>0.451824</td>\n      <td>0.418870</td>\n      <td>0.523571</td>\n      <td>0.667841</td>\n      <td>0.349998</td>\n      <td>0.398406</td>\n      <td>0.283049</td>\n      <td>0.357295</td>\n      <td>0.196558</td>\n      <td>0.622334</td>\n      <td>0.402972</td>\n      <td>0.644132</td>\n      <td>0.505381</td>\n      <td>0.516290</td>\n      <td>0.701324</td>\n      <td>0.556068</td>\n      <td>0.543404</td>\n      <td>0.296463</td>\n      <td>0.562427</td>\n      <td>0.525061</td>\n      <td>0.590872</td>\n      <td>0.568036</td>\n      <td>0.540439</td>\n      <td>-0.732289</td>\n      <td>-0.731461</td>\n      <td>-0.727720</td>\n      <td>-0.725979</td>\n      <td>-0.730731</td>\n    </tr>\n    <tr>\n      <th>ShortPassing</th>\n      <td>0.296287</td>\n      <td>0.132018</td>\n      <td>0.499904</td>\n      <td>0.368916</td>\n      <td>0.326873</td>\n      <td>-0.357552</td>\n      <td>-0.283021</td>\n      <td>0.813317</td>\n      <td>0.667675</td>\n      <td>0.650725</td>\n      <td>1.000000</td>\n      <td>0.703631</td>\n      <td>0.847526</td>\n      <td>0.779525</td>\n      <td>0.741013</td>\n      <td>0.898834</td>\n      <td>0.914923</td>\n      <td>0.584460</td>\n      <td>0.574006</td>\n      <td>0.629232</td>\n      <td>0.515189</td>\n      <td>0.554252</td>\n      <td>0.778944</td>\n      <td>0.241976</td>\n      <td>0.727659</td>\n      <td>0.179505</td>\n      <td>0.765440</td>\n      <td>0.625008</td>\n      <td>0.552991</td>\n      <td>0.762505</td>\n      <td>0.724392</td>\n      <td>0.685726</td>\n      <td>0.701340</td>\n      <td>0.569873</td>\n      <td>0.550521</td>\n      <td>0.518118</td>\n      <td>-0.703716</td>\n      <td>-0.701893</td>\n      <td>-0.697862</td>\n      <td>-0.697495</td>\n      <td>-0.702814</td>\n    </tr>\n    <tr>\n      <th>Volleys</th>\n      <td>0.259252</td>\n      <td>0.142423</td>\n      <td>0.392951</td>\n      <td>0.257102</td>\n      <td>0.290156</td>\n      <td>-0.345871</td>\n      <td>-0.260315</td>\n      <td>0.695608</td>\n      <td>0.884473</td>\n      <td>0.515015</td>\n      <td>0.703631</td>\n      <td>1.000000</td>\n      <td>0.812841</td>\n      <td>0.809694</td>\n      <td>0.753334</td>\n      <td>0.579671</td>\n      <td>0.798123</td>\n      <td>0.580793</td>\n      <td>0.566128</td>\n      <td>0.631961</td>\n      <td>0.411213</td>\n      <td>0.524656</td>\n      <td>0.835336</td>\n      <td>0.154566</td>\n      <td>0.536922</td>\n      <td>0.060482</td>\n      <td>0.870139</td>\n      <td>0.344829</td>\n      <td>0.102011</td>\n      <td>0.850916</td>\n      <td>0.704398</td>\n      <td>0.833141</td>\n      <td>0.603296</td>\n      <td>0.135439</td>\n      <td>0.086371</td>\n      <td>0.048595</td>\n      <td>-0.579121</td>\n      <td>-0.577077</td>\n      <td>-0.573309</td>\n      <td>-0.574434</td>\n      <td>-0.577288</td>\n    </tr>\n    <tr>\n      <th>Dribbling</th>\n      <td>0.238315</td>\n      <td>0.011541</td>\n      <td>0.374591</td>\n      <td>0.317479</td>\n      <td>0.273076</td>\n      <td>-0.486929</td>\n      <td>-0.408437</td>\n      <td>0.859764</td>\n      <td>0.826993</td>\n      <td>0.561127</td>\n      <td>0.847526</td>\n      <td>0.812841</td>\n      <td>1.000000</td>\n      <td>0.845635</td>\n      <td>0.757763</td>\n      <td>0.729923</td>\n      <td>0.940201</td>\n      <td>0.754575</td>\n      <td>0.733831</td>\n      <td>0.770942</td>\n      <td>0.395887</td>\n      <td>0.672702</td>\n      <td>0.809628</td>\n      <td>0.178042</td>\n      <td>0.694809</td>\n      <td>0.007214</td>\n      <td>0.846276</td>\n      <td>0.456431</td>\n      <td>0.308926</td>\n      <td>0.898585</td>\n      <td>0.736619</td>\n      <td>0.775196</td>\n      <td>0.610364</td>\n      <td>0.349305</td>\n      <td>0.313817</td>\n      <td>0.286320</td>\n      <td>-0.737743</td>\n      <td>-0.736281</td>\n      <td>-0.732708</td>\n      <td>-0.734391</td>\n      <td>-0.737654</td>\n    </tr>\n    <tr>\n      <th>Curve</th>\n      <td>0.260829</td>\n      <td>0.143599</td>\n      <td>0.420967</td>\n      <td>0.281499</td>\n      <td>0.288518</td>\n      <td>-0.436651</td>\n      <td>-0.342680</td>\n      <td>0.837136</td>\n      <td>0.762459</td>\n      <td>0.451824</td>\n      <td>0.779525</td>\n      <td>0.809694</td>\n      <td>0.845635</td>\n      <td>1.000000</td>\n      <td>0.863569</td>\n      <td>0.717701</td>\n      <td>0.832536</td>\n      <td>0.616131</td>\n      <td>0.588593</td>\n      <td>0.688305</td>\n      <td>0.431137</td>\n      <td>0.597053</td>\n      <td>0.795924</td>\n      <td>0.143184</td>\n      <td>0.599638</td>\n      <td>-0.000667</td>\n      <td>0.837311</td>\n      <td>0.413465</td>\n      <td>0.286178</td>\n      <td>0.814364</td>\n      <td>0.749207</td>\n      <td>0.756885</td>\n      <td>0.624992</td>\n      <td>0.303079</td>\n      <td>0.274199</td>\n      <td>0.245469</td>\n      <td>-0.593628</td>\n      <td>-0.590646</td>\n      <td>-0.587498</td>\n      <td>-0.590877</td>\n      <td>-0.592644</td>\n    </tr>\n    <tr>\n      <th>FKAccuracy</th>\n      <td>0.238217</td>\n      <td>0.193484</td>\n      <td>0.398798</td>\n      <td>0.232954</td>\n      <td>0.267599</td>\n      <td>-0.400265</td>\n      <td>-0.301647</td>\n      <td>0.765394</td>\n      <td>0.701886</td>\n      <td>0.418870</td>\n      <td>0.741013</td>\n      <td>0.753334</td>\n      <td>0.757763</td>\n      <td>0.863569</td>\n      <td>1.000000</td>\n      <td>0.709906</td>\n      <td>0.763696</td>\n      <td>0.509074</td>\n      <td>0.478839</td>\n      <td>0.598157</td>\n      <td>0.415244</td>\n      <td>0.532279</td>\n      <td>0.758717</td>\n      <td>0.114067</td>\n      <td>0.546884</td>\n      <td>0.014608</td>\n      <td>0.805604</td>\n      <td>0.409614</td>\n      <td>0.306554</td>\n      <td>0.733721</td>\n      <td>0.722136</td>\n      <td>0.739379</td>\n      <td>0.593813</td>\n      <td>0.310198</td>\n      <td>0.290496</td>\n      <td>0.259141</td>\n      <td>-0.544894</td>\n      <td>-0.541930</td>\n      <td>-0.537909</td>\n      <td>-0.540718</td>\n      <td>-0.543333</td>\n    </tr>\n    <tr>\n      <th>LongPassing</th>\n      <td>0.277910</td>\n      <td>0.180289</td>\n      <td>0.482774</td>\n      <td>0.322378</td>\n      <td>0.303158</td>\n      <td>-0.325218</td>\n      <td>-0.255799</td>\n      <td>0.762133</td>\n      <td>0.522642</td>\n      <td>0.523571</td>\n      <td>0.898834</td>\n      <td>0.579671</td>\n      <td>0.729923</td>\n      <td>0.717701</td>\n      <td>0.709906</td>\n      <td>1.000000</td>\n      <td>0.795900</td>\n      <td>0.462958</td>\n      <td>0.447707</td>\n      <td>0.540945</td>\n      <td>0.487690</td>\n      <td>0.483199</td>\n      <td>0.681522</td>\n      <td>0.194380</td>\n      <td>0.647276</td>\n      <td>0.154642</td>\n      <td>0.674899</td>\n      <td>0.602342</td>\n      <td>0.604220</td>\n      <td>0.623370</td>\n      <td>0.708672</td>\n      <td>0.555080</td>\n      <td>0.659748</td>\n      <td>0.595358</td>\n      <td>0.594717</td>\n      <td>0.569450</td>\n      <td>-0.577847</td>\n      <td>-0.575926</td>\n      <td>-0.572071</td>\n      <td>-0.572353</td>\n      <td>-0.576941</td>\n    </tr>\n    <tr>\n      <th>BallControl</th>\n      <td>0.277971</td>\n      <td>0.085402</td>\n      <td>0.460365</td>\n      <td>0.355717</td>\n      <td>0.308977</td>\n      <td>-0.408868</td>\n      <td>-0.331395</td>\n      <td>0.844315</td>\n      <td>0.791308</td>\n      <td>0.667841</td>\n      <td>0.914923</td>\n      <td>0.798123</td>\n      <td>0.940201</td>\n      <td>0.832536</td>\n      <td>0.763696</td>\n      <td>0.795900</td>\n      <td>1.000000</td>\n      <td>0.687606</td>\n      <td>0.676341</td>\n      <td>0.715133</td>\n      <td>0.473259</td>\n      <td>0.616656</td>\n      <td>0.836555</td>\n      <td>0.234598</td>\n      <td>0.738188</td>\n      <td>0.130016</td>\n      <td>0.838438</td>\n      <td>0.564785</td>\n      <td>0.431224</td>\n      <td>0.866036</td>\n      <td>0.727291</td>\n      <td>0.776198</td>\n      <td>0.688709</td>\n      <td>0.465785</td>\n      <td>0.430032</td>\n      <td>0.397209</td>\n      <td>-0.766158</td>\n      <td>-0.764445</td>\n      <td>-0.760827</td>\n      <td>-0.761163</td>\n      <td>-0.765924</td>\n    </tr>\n    <tr>\n      <th>Acceleration</th>\n      <td>0.127312</td>\n      <td>-0.154191</td>\n      <td>0.199887</td>\n      <td>0.238047</td>\n      <td>0.172122</td>\n      <td>-0.531553</td>\n      <td>-0.465779</td>\n      <td>0.675733</td>\n      <td>0.613557</td>\n      <td>0.349998</td>\n      <td>0.584460</td>\n      <td>0.580793</td>\n      <td>0.754575</td>\n      <td>0.616131</td>\n      <td>0.509074</td>\n      <td>0.462958</td>\n      <td>0.687606</td>\n      <td>1.000000</td>\n      <td>0.925596</td>\n      <td>0.819908</td>\n      <td>0.247078</td>\n      <td>0.725896</td>\n      <td>0.555020</td>\n      <td>0.263338</td>\n      <td>0.624012</td>\n      <td>-0.101243</td>\n      <td>0.588569</td>\n      <td>0.276687</td>\n      <td>0.172210</td>\n      <td>0.688217</td>\n      <td>0.483549</td>\n      <td>0.547733</td>\n      <td>0.383688</td>\n      <td>0.215240</td>\n      <td>0.181819</td>\n      <td>0.175950</td>\n      <td>-0.566592</td>\n      <td>-0.567988</td>\n      <td>-0.565077</td>\n      <td>-0.565363</td>\n      <td>-0.567027</td>\n    </tr>\n    <tr>\n      <th>SprintSpeed</th>\n      <td>0.132349</td>\n      <td>-0.146779</td>\n      <td>0.213534</td>\n      <td>0.239984</td>\n      <td>0.173931</td>\n      <td>-0.452834</td>\n      <td>-0.399840</td>\n      <td>0.653687</td>\n      <td>0.601499</td>\n      <td>0.398406</td>\n      <td>0.574006</td>\n      <td>0.566128</td>\n      <td>0.733831</td>\n      <td>0.588593</td>\n      <td>0.478839</td>\n      <td>0.447707</td>\n      <td>0.676341</td>\n      <td>0.925596</td>\n      <td>1.000000</td>\n      <td>0.775135</td>\n      <td>0.251993</td>\n      <td>0.661363</td>\n      <td>0.560175</td>\n      <td>0.279875</td>\n      <td>0.636321</td>\n      <td>-0.021514</td>\n      <td>0.570475</td>\n      <td>0.304117</td>\n      <td>0.183772</td>\n      <td>0.671696</td>\n      <td>0.453312</td>\n      <td>0.536504</td>\n      <td>0.388814</td>\n      <td>0.231942</td>\n      <td>0.196553</td>\n      <td>0.189906</td>\n      <td>-0.570151</td>\n      <td>-0.571603</td>\n      <td>-0.569148</td>\n      <td>-0.568525</td>\n      <td>-0.570517</td>\n    </tr>\n    <tr>\n      <th>Agility</th>\n      <td>0.157302</td>\n      <td>-0.017642</td>\n      <td>0.266740</td>\n      <td>0.225557</td>\n      <td>0.194676</td>\n      <td>-0.606220</td>\n      <td>-0.521236</td>\n      <td>0.704819</td>\n      <td>0.650450</td>\n      <td>0.283049</td>\n      <td>0.629232</td>\n      <td>0.631961</td>\n      <td>0.770942</td>\n      <td>0.688305</td>\n      <td>0.598157</td>\n      <td>0.540945</td>\n      <td>0.715133</td>\n      <td>0.819908</td>\n      <td>0.775135</td>\n      <td>1.000000</td>\n      <td>0.326891</td>\n      <td>0.782358</td>\n      <td>0.587873</td>\n      <td>0.262609</td>\n      <td>0.586842</td>\n      <td>-0.165643</td>\n      <td>0.651534</td>\n      <td>0.267279</td>\n      <td>0.159105</td>\n      <td>0.713531</td>\n      <td>0.613975</td>\n      <td>0.579791</td>\n      <td>0.463948</td>\n      <td>0.187999</td>\n      <td>0.149024</td>\n      <td>0.136166</td>\n      <td>-0.502715</td>\n      <td>-0.503145</td>\n      <td>-0.501580</td>\n      <td>-0.501704</td>\n      <td>-0.504113</td>\n    </tr>\n    <tr>\n      <th>Reactions</th>\n      <td>0.477211</td>\n      <td>0.427635</td>\n      <td>0.812473</td>\n      <td>0.496197</td>\n      <td>0.519866</td>\n      <td>-0.017717</td>\n      <td>0.082039</td>\n      <td>0.410975</td>\n      <td>0.351554</td>\n      <td>0.357295</td>\n      <td>0.515189</td>\n      <td>0.411213</td>\n      <td>0.395887</td>\n      <td>0.431137</td>\n      <td>0.415244</td>\n      <td>0.487690</td>\n      <td>0.473259</td>\n      <td>0.247078</td>\n      <td>0.251993</td>\n      <td>0.326891</td>\n      <td>1.000000</td>\n      <td>0.214262</td>\n      <td>0.445834</td>\n      <td>0.323820</td>\n      <td>0.409812</td>\n      <td>0.348889</td>\n      <td>0.437301</td>\n      <td>0.431116</td>\n      <td>0.356473</td>\n      <td>0.406754</td>\n      <td>0.530670</td>\n      <td>0.376308</td>\n      <td>0.712632</td>\n      <td>0.307710</td>\n      <td>0.278040</td>\n      <td>0.251614</td>\n      <td>-0.043725</td>\n      <td>-0.042483</td>\n      <td>-0.045894</td>\n      <td>-0.035819</td>\n      <td>-0.041116</td>\n    </tr>\n    <tr>\n      <th>Balance</th>\n      <td>0.091619</td>\n      <td>-0.086890</td>\n      <td>0.108357</td>\n      <td>0.143630</td>\n      <td>0.115995</td>\n      <td>-0.762855</td>\n      <td>-0.645334</td>\n      <td>0.627755</td>\n      <td>0.533800</td>\n      <td>0.196558</td>\n      <td>0.554252</td>\n      <td>0.524656</td>\n      <td>0.672702</td>\n      <td>0.597053</td>\n      <td>0.532279</td>\n      <td>0.483199</td>\n      <td>0.616656</td>\n      <td>0.725896</td>\n      <td>0.661363</td>\n      <td>0.782358</td>\n      <td>0.214262</td>\n      <td>1.000000</td>\n      <td>0.477379</td>\n      <td>0.241015</td>\n      <td>0.498322</td>\n      <td>-0.308757</td>\n      <td>0.543796</td>\n      <td>0.215509</td>\n      <td>0.172065</td>\n      <td>0.605203</td>\n      <td>0.513511</td>\n      <td>0.500145</td>\n      <td>0.351383</td>\n      <td>0.200620</td>\n      <td>0.174597</td>\n      <td>0.172708</td>\n      <td>-0.479692</td>\n      <td>-0.480854</td>\n      <td>-0.478671</td>\n      <td>-0.478434</td>\n      <td>-0.481105</td>\n    </tr>\n    <tr>\n      <th>ShotPower</th>\n      <td>0.259531</td>\n      <td>0.156358</td>\n      <td>0.442167</td>\n      <td>0.290572</td>\n      <td>0.282451</td>\n      <td>-0.286300</td>\n      <td>-0.187543</td>\n      <td>0.712821</td>\n      <td>0.818319</td>\n      <td>0.622334</td>\n      <td>0.778944</td>\n      <td>0.835336</td>\n      <td>0.809628</td>\n      <td>0.795924</td>\n      <td>0.758717</td>\n      <td>0.681522</td>\n      <td>0.836555</td>\n      <td>0.555020</td>\n      <td>0.560175</td>\n      <td>0.587873</td>\n      <td>0.445834</td>\n      <td>0.477379</td>\n      <td>1.000000</td>\n      <td>0.221465</td>\n      <td>0.628771</td>\n      <td>0.204837</td>\n      <td>0.890791</td>\n      <td>0.507299</td>\n      <td>0.279940</td>\n      <td>0.813162</td>\n      <td>0.689388</td>\n      <td>0.800470</td>\n      <td>0.647903</td>\n      <td>0.312568</td>\n      <td>0.271286</td>\n      <td>0.234967</td>\n      <td>-0.636334</td>\n      <td>-0.636189</td>\n      <td>-0.631316</td>\n      <td>-0.633348</td>\n      <td>-0.635971</td>\n    </tr>\n    <tr>\n      <th>Jumping</th>\n      <td>0.129796</td>\n      <td>0.172238</td>\n      <td>0.263994</td>\n      <td>0.114496</td>\n      <td>0.124827</td>\n      <td>-0.065480</td>\n      <td>0.009483</td>\n      <td>0.167838</td>\n      <td>0.126284</td>\n      <td>0.402972</td>\n      <td>0.241976</td>\n      <td>0.154566</td>\n      <td>0.178042</td>\n      <td>0.143184</td>\n      <td>0.114067</td>\n      <td>0.194380</td>\n      <td>0.234598</td>\n      <td>0.263338</td>\n      <td>0.279875</td>\n      <td>0.262609</td>\n      <td>0.323820</td>\n      <td>0.241015</td>\n      <td>0.221465</td>\n      <td>1.000000</td>\n      <td>0.382232</td>\n      <td>0.332519</td>\n      <td>0.164618</td>\n      <td>0.398800</td>\n      <td>0.307693</td>\n      <td>0.174225</td>\n      <td>0.109636</td>\n      <td>0.169999</td>\n      <td>0.305408</td>\n      <td>0.300401</td>\n      <td>0.279959</td>\n      <td>0.278948</td>\n      <td>-0.171809</td>\n      <td>-0.172220</td>\n      <td>-0.173600</td>\n      <td>-0.167770</td>\n      <td>-0.171295</td>\n    </tr>\n    <tr>\n      <th>Stamina</th>\n      <td>0.179546</td>\n      <td>0.097799</td>\n      <td>0.365773</td>\n      <td>0.205499</td>\n      <td>0.212278</td>\n      <td>-0.280670</td>\n      <td>-0.216783</td>\n      <td>0.679565</td>\n      <td>0.520971</td>\n      <td>0.644132</td>\n      <td>0.727659</td>\n      <td>0.536922</td>\n      <td>0.694809</td>\n      <td>0.599638</td>\n      <td>0.546884</td>\n      <td>0.647276</td>\n      <td>0.738188</td>\n      <td>0.624012</td>\n      <td>0.636321</td>\n      <td>0.586842</td>\n      <td>0.409812</td>\n      <td>0.498322</td>\n      <td>0.628771</td>\n      <td>0.382232</td>\n      <td>1.000000</td>\n      <td>0.301892</td>\n      <td>0.604667</td>\n      <td>0.657313</td>\n      <td>0.584096</td>\n      <td>0.648854</td>\n      <td>0.492403</td>\n      <td>0.530534</td>\n      <td>0.548025</td>\n      <td>0.596170</td>\n      <td>0.577118</td>\n      <td>0.551795</td>\n      <td>-0.675158</td>\n      <td>-0.672093</td>\n      <td>-0.670007</td>\n      <td>-0.669530</td>\n      <td>-0.673544</td>\n    </tr>\n    <tr>\n      <th>Strength</th>\n      <td>0.140250</td>\n      <td>0.323395</td>\n      <td>0.347049</td>\n      <td>0.082126</td>\n      <td>0.130114</td>\n      <td>0.519396</td>\n      <td>0.596124</td>\n      <td>0.007803</td>\n      <td>0.022010</td>\n      <td>0.505381</td>\n      <td>0.179505</td>\n      <td>0.060482</td>\n      <td>0.007214</td>\n      <td>-0.000667</td>\n      <td>0.014608</td>\n      <td>0.154642</td>\n      <td>0.130016</td>\n      <td>-0.101243</td>\n      <td>-0.021514</td>\n      <td>-0.165643</td>\n      <td>0.348889</td>\n      <td>-0.308757</td>\n      <td>0.204837</td>\n      <td>0.332519</td>\n      <td>0.301892</td>\n      <td>1.000000</td>\n      <td>0.080933</td>\n      <td>0.493954</td>\n      <td>0.372525</td>\n      <td>0.041579</td>\n      <td>0.005181</td>\n      <td>0.092813</td>\n      <td>0.330239</td>\n      <td>0.351767</td>\n      <td>0.348424</td>\n      <td>0.321084</td>\n      <td>-0.093647</td>\n      <td>-0.091811</td>\n      <td>-0.091948</td>\n      <td>-0.086262</td>\n      <td>-0.090339</td>\n    </tr>\n    <tr>\n      <th>LongShots</th>\n      <td>0.251102</td>\n      <td>0.155028</td>\n      <td>0.422599</td>\n      <td>0.268951</td>\n      <td>0.281763</td>\n      <td>-0.377344</td>\n      <td>-0.275134</td>\n      <td>0.746821</td>\n      <td>0.879485</td>\n      <td>0.516290</td>\n      <td>0.765440</td>\n      <td>0.870139</td>\n      <td>0.846276</td>\n      <td>0.837311</td>\n      <td>0.805604</td>\n      <td>0.674899</td>\n      <td>0.838438</td>\n      <td>0.588569</td>\n      <td>0.570475</td>\n      <td>0.651534</td>\n      <td>0.437301</td>\n      <td>0.543796</td>\n      <td>0.890791</td>\n      <td>0.164618</td>\n      <td>0.604667</td>\n      <td>0.080933</td>\n      <td>1.000000</td>\n      <td>0.406138</td>\n      <td>0.206204</td>\n      <td>0.863438</td>\n      <td>0.757464</td>\n      <td>0.815938</td>\n      <td>0.623534</td>\n      <td>0.228920</td>\n      <td>0.185134</td>\n      <td>0.146129</td>\n      <td>-0.600338</td>\n      <td>-0.598725</td>\n      <td>-0.593847</td>\n      <td>-0.595083</td>\n      <td>-0.598392</td>\n    </tr>\n    <tr>\n      <th>Aggression</th>\n      <td>0.196484</td>\n      <td>0.263356</td>\n      <td>0.396327</td>\n      <td>0.173991</td>\n      <td>0.186601</td>\n      <td>-0.042109</td>\n      <td>0.031942</td>\n      <td>0.485786</td>\n      <td>0.259058</td>\n      <td>0.701324</td>\n      <td>0.625008</td>\n      <td>0.344829</td>\n      <td>0.456431</td>\n      <td>0.413465</td>\n      <td>0.409614</td>\n      <td>0.602342</td>\n      <td>0.564785</td>\n      <td>0.276687</td>\n      <td>0.304117</td>\n      <td>0.267279</td>\n      <td>0.431116</td>\n      <td>0.215509</td>\n      <td>0.507299</td>\n      <td>0.398800</td>\n      <td>0.657313</td>\n      <td>0.493954</td>\n      <td>0.406138</td>\n      <td>1.000000</td>\n      <td>0.755429</td>\n      <td>0.396618</td>\n      <td>0.322492</td>\n      <td>0.354806</td>\n      <td>0.535170</td>\n      <td>0.729088</td>\n      <td>0.747817</td>\n      <td>0.724999</td>\n      <td>-0.559896</td>\n      <td>-0.559764</td>\n      <td>-0.556878</td>\n      <td>-0.554751</td>\n      <td>-0.559176</td>\n    </tr>\n    <tr>\n      <th>Interceptions</th>\n      <td>0.159995</td>\n      <td>0.197649</td>\n      <td>0.324114</td>\n      <td>0.158222</td>\n      <td>0.143222</td>\n      <td>-0.049344</td>\n      <td>-0.024375</td>\n      <td>0.436860</td>\n      <td>-0.006059</td>\n      <td>0.556068</td>\n      <td>0.552991</td>\n      <td>0.102011</td>\n      <td>0.308926</td>\n      <td>0.286178</td>\n      <td>0.306554</td>\n      <td>0.604220</td>\n      <td>0.431224</td>\n      <td>0.172210</td>\n      <td>0.183772</td>\n      <td>0.159105</td>\n      <td>0.356473</td>\n      <td>0.172065</td>\n      <td>0.279940</td>\n      <td>0.307693</td>\n      <td>0.584096</td>\n      <td>0.372525</td>\n      <td>0.206204</td>\n      <td>0.755429</td>\n      <td>1.000000</td>\n      <td>0.184507</td>\n      <td>0.201596</td>\n      <td>0.126989</td>\n      <td>0.412375</td>\n      <td>0.890207</td>\n      <td>0.942202</td>\n      <td>0.929105</td>\n      <td>-0.475640</td>\n      <td>-0.476089</td>\n      <td>-0.474824</td>\n      <td>-0.470975</td>\n      <td>-0.475956</td>\n    </tr>\n    <tr>\n      <th>Positioning</th>\n      <td>0.228335</td>\n      <td>0.083854</td>\n      <td>0.359001</td>\n      <td>0.248104</td>\n      <td>0.260952</td>\n      <td>-0.430577</td>\n      <td>-0.346132</td>\n      <td>0.787018</td>\n      <td>0.890732</td>\n      <td>0.543404</td>\n      <td>0.762505</td>\n      <td>0.850916</td>\n      <td>0.898585</td>\n      <td>0.814364</td>\n      <td>0.733721</td>\n      <td>0.623370</td>\n      <td>0.866036</td>\n      <td>0.688217</td>\n      <td>0.671696</td>\n      <td>0.713531</td>\n      <td>0.406754</td>\n      <td>0.605203</td>\n      <td>0.813162</td>\n      <td>0.174225</td>\n      <td>0.648854</td>\n      <td>0.041579</td>\n      <td>0.863438</td>\n      <td>0.396618</td>\n      <td>0.184507</td>\n      <td>1.000000</td>\n      <td>0.739196</td>\n      <td>0.805887</td>\n      <td>0.590992</td>\n      <td>0.216638</td>\n      <td>0.171454</td>\n      <td>0.137384</td>\n      <td>-0.666150</td>\n      <td>-0.664461</td>\n      <td>-0.660880</td>\n      <td>-0.662192</td>\n      <td>-0.665543</td>\n    </tr>\n    <tr>\n      <th>Vision</th>\n      <td>0.314749</td>\n      <td>0.185797</td>\n      <td>0.497630</td>\n      <td>0.348328</td>\n      <td>0.356646</td>\n      <td>-0.360994</td>\n      <td>-0.277647</td>\n      <td>0.692419</td>\n      <td>0.702071</td>\n      <td>0.296463</td>\n      <td>0.724392</td>\n      <td>0.704398</td>\n      <td>0.736619</td>\n      <td>0.749207</td>\n      <td>0.722136</td>\n      <td>0.708672</td>\n      <td>0.727291</td>\n      <td>0.483549</td>\n      <td>0.453312</td>\n      <td>0.613975</td>\n      <td>0.530670</td>\n      <td>0.513511</td>\n      <td>0.689388</td>\n      <td>0.109636</td>\n      <td>0.492403</td>\n      <td>0.005181</td>\n      <td>0.757464</td>\n      <td>0.322492</td>\n      <td>0.201596</td>\n      <td>0.739196</td>\n      <td>1.000000</td>\n      <td>0.644031</td>\n      <td>0.652474</td>\n      <td>0.196451</td>\n      <td>0.165261</td>\n      <td>0.131883</td>\n      <td>-0.363215</td>\n      <td>-0.359291</td>\n      <td>-0.355750</td>\n      <td>-0.357017</td>\n      <td>-0.362782</td>\n    </tr>\n    <tr>\n      <th>Penalties</th>\n      <td>0.224685</td>\n      <td>0.138605</td>\n      <td>0.342878</td>\n      <td>0.227321</td>\n      <td>0.241207</td>\n      <td>-0.335078</td>\n      <td>-0.249198</td>\n      <td>0.653712</td>\n      <td>0.840845</td>\n      <td>0.562427</td>\n      <td>0.685726</td>\n      <td>0.833141</td>\n      <td>0.775196</td>\n      <td>0.756885</td>\n      <td>0.739379</td>\n      <td>0.555080</td>\n      <td>0.776198</td>\n      <td>0.547733</td>\n      <td>0.536504</td>\n      <td>0.579791</td>\n      <td>0.376308</td>\n      <td>0.500145</td>\n      <td>0.800470</td>\n      <td>0.169999</td>\n      <td>0.530534</td>\n      <td>0.092813</td>\n      <td>0.815938</td>\n      <td>0.354806</td>\n      <td>0.126989</td>\n      <td>0.805887</td>\n      <td>0.644031</td>\n      <td>1.000000</td>\n      <td>0.567862</td>\n      <td>0.169029</td>\n      <td>0.117891</td>\n      <td>0.082436</td>\n      <td>-0.603583</td>\n      <td>-0.602283</td>\n      <td>-0.597264</td>\n      <td>-0.600452</td>\n      <td>-0.602818</td>\n    </tr>\n    <tr>\n      <th>Composure</th>\n      <td>0.413919</td>\n      <td>0.379930</td>\n      <td>0.713420</td>\n      <td>0.434466</td>\n      <td>0.443918</td>\n      <td>-0.129665</td>\n      <td>-0.032196</td>\n      <td>0.587027</td>\n      <td>0.543090</td>\n      <td>0.525061</td>\n      <td>0.701340</td>\n      <td>0.603296</td>\n      <td>0.610364</td>\n      <td>0.624992</td>\n      <td>0.593813</td>\n      <td>0.659748</td>\n      <td>0.688709</td>\n      <td>0.383688</td>\n      <td>0.388814</td>\n      <td>0.463948</td>\n      <td>0.712632</td>\n      <td>0.351383</td>\n      <td>0.647903</td>\n      <td>0.305408</td>\n      <td>0.548025</td>\n      <td>0.330239</td>\n      <td>0.623534</td>\n      <td>0.535170</td>\n      <td>0.412375</td>\n      <td>0.590992</td>\n      <td>0.652474</td>\n      <td>0.567862</td>\n      <td>1.000000</td>\n      <td>0.401127</td>\n      <td>0.367515</td>\n      <td>0.333513</td>\n      <td>-0.352608</td>\n      <td>-0.349402</td>\n      <td>-0.348168</td>\n      <td>-0.344019</td>\n      <td>-0.351831</td>\n    </tr>\n    <tr>\n      <th>Marking</th>\n      <td>0.148582</td>\n      <td>0.143118</td>\n      <td>0.289565</td>\n      <td>0.166234</td>\n      <td>0.136821</td>\n      <td>-0.072609</td>\n      <td>-0.047815</td>\n      <td>0.452757</td>\n      <td>0.039418</td>\n      <td>0.590872</td>\n      <td>0.569873</td>\n      <td>0.135439</td>\n      <td>0.349305</td>\n      <td>0.303079</td>\n      <td>0.310198</td>\n      <td>0.595358</td>\n      <td>0.465785</td>\n      <td>0.215240</td>\n      <td>0.231942</td>\n      <td>0.187999</td>\n      <td>0.307710</td>\n      <td>0.200620</td>\n      <td>0.312568</td>\n      <td>0.300401</td>\n      <td>0.596170</td>\n      <td>0.351767</td>\n      <td>0.228920</td>\n      <td>0.729088</td>\n      <td>0.890207</td>\n      <td>0.216638</td>\n      <td>0.196451</td>\n      <td>0.169029</td>\n      <td>0.401127</td>\n      <td>1.000000</td>\n      <td>0.907658</td>\n      <td>0.897192</td>\n      <td>-0.539977</td>\n      <td>-0.540849</td>\n      <td>-0.538036</td>\n      <td>-0.535412</td>\n      <td>-0.540356</td>\n    </tr>\n    <tr>\n      <th>StandingTackle</th>\n      <td>0.129007</td>\n      <td>0.119987</td>\n      <td>0.255716</td>\n      <td>0.147085</td>\n      <td>0.111076</td>\n      <td>-0.057870</td>\n      <td>-0.045426</td>\n      <td>0.437817</td>\n      <td>-0.018439</td>\n      <td>0.568036</td>\n      <td>0.550521</td>\n      <td>0.086371</td>\n      <td>0.313817</td>\n      <td>0.274199</td>\n      <td>0.290496</td>\n      <td>0.594717</td>\n      <td>0.430032</td>\n      <td>0.181819</td>\n      <td>0.196553</td>\n      <td>0.149024</td>\n      <td>0.278040</td>\n      <td>0.174597</td>\n      <td>0.271286</td>\n      <td>0.279959</td>\n      <td>0.577118</td>\n      <td>0.348424</td>\n      <td>0.185134</td>\n      <td>0.747817</td>\n      <td>0.942202</td>\n      <td>0.171454</td>\n      <td>0.165261</td>\n      <td>0.117891</td>\n      <td>0.367515</td>\n      <td>0.907658</td>\n      <td>1.000000</td>\n      <td>0.974926</td>\n      <td>-0.521123</td>\n      <td>-0.521956</td>\n      <td>-0.520658</td>\n      <td>-0.517581</td>\n      <td>-0.521492</td>\n    </tr>\n    <tr>\n      <th>SlidingTackle</th>\n      <td>0.113800</td>\n      <td>0.103082</td>\n      <td>0.225634</td>\n      <td>0.132381</td>\n      <td>0.090444</td>\n      <td>-0.066151</td>\n      <td>-0.054921</td>\n      <td>0.418693</td>\n      <td>-0.057634</td>\n      <td>0.540439</td>\n      <td>0.518118</td>\n      <td>0.048595</td>\n      <td>0.286320</td>\n      <td>0.245469</td>\n      <td>0.259141</td>\n      <td>0.569450</td>\n      <td>0.397209</td>\n      <td>0.175950</td>\n      <td>0.189906</td>\n      <td>0.136166</td>\n      <td>0.251614</td>\n      <td>0.172708</td>\n      <td>0.234967</td>\n      <td>0.278948</td>\n      <td>0.551795</td>\n      <td>0.321084</td>\n      <td>0.146129</td>\n      <td>0.724999</td>\n      <td>0.929105</td>\n      <td>0.137384</td>\n      <td>0.131883</td>\n      <td>0.082436</td>\n      <td>0.333513</td>\n      <td>0.897192</td>\n      <td>0.974926</td>\n      <td>1.000000</td>\n      <td>-0.499579</td>\n      <td>-0.500562</td>\n      <td>-0.499096</td>\n      <td>-0.495664</td>\n      <td>-0.499555</td>\n    </tr>\n    <tr>\n      <th>GKDiving</th>\n      <td>-0.024234</td>\n      <td>0.099971</td>\n      <td>-0.024982</td>\n      <td>-0.051163</td>\n      <td>-0.027272</td>\n      <td>0.360583</td>\n      <td>0.338844</td>\n      <td>-0.648413</td>\n      <td>-0.577767</td>\n      <td>-0.732289</td>\n      <td>-0.703716</td>\n      <td>-0.579121</td>\n      <td>-0.737743</td>\n      <td>-0.593628</td>\n      <td>-0.544894</td>\n      <td>-0.577847</td>\n      <td>-0.766158</td>\n      <td>-0.566592</td>\n      <td>-0.570151</td>\n      <td>-0.502715</td>\n      <td>-0.043725</td>\n      <td>-0.479692</td>\n      <td>-0.636334</td>\n      <td>-0.171809</td>\n      <td>-0.675158</td>\n      <td>-0.093647</td>\n      <td>-0.600338</td>\n      <td>-0.559896</td>\n      <td>-0.475640</td>\n      <td>-0.666150</td>\n      <td>-0.363215</td>\n      <td>-0.603583</td>\n      <td>-0.352608</td>\n      <td>-0.539977</td>\n      <td>-0.521123</td>\n      <td>-0.499579</td>\n      <td>1.000000</td>\n      <td>0.970220</td>\n      <td>0.965644</td>\n      <td>0.969816</td>\n      <td>0.973334</td>\n    </tr>\n    <tr>\n      <th>GKHandling</th>\n      <td>-0.023797</td>\n      <td>0.105088</td>\n      <td>-0.024332</td>\n      <td>-0.052652</td>\n      <td>-0.027555</td>\n      <td>0.360914</td>\n      <td>0.338018</td>\n      <td>-0.645549</td>\n      <td>-0.576122</td>\n      <td>-0.731461</td>\n      <td>-0.701893</td>\n      <td>-0.577077</td>\n      <td>-0.736281</td>\n      <td>-0.590646</td>\n      <td>-0.541930</td>\n      <td>-0.575926</td>\n      <td>-0.764445</td>\n      <td>-0.567988</td>\n      <td>-0.571603</td>\n      <td>-0.503145</td>\n      <td>-0.042483</td>\n      <td>-0.480854</td>\n      <td>-0.636189</td>\n      <td>-0.172220</td>\n      <td>-0.672093</td>\n      <td>-0.091811</td>\n      <td>-0.598725</td>\n      <td>-0.559764</td>\n      <td>-0.476089</td>\n      <td>-0.664461</td>\n      <td>-0.359291</td>\n      <td>-0.602283</td>\n      <td>-0.349402</td>\n      <td>-0.540849</td>\n      <td>-0.521956</td>\n      <td>-0.500562</td>\n      <td>0.970220</td>\n      <td>1.000000</td>\n      <td>0.965169</td>\n      <td>0.969420</td>\n      <td>0.970251</td>\n    </tr>\n    <tr>\n      <th>GKKicking</th>\n      <td>-0.026884</td>\n      <td>0.103533</td>\n      <td>-0.028483</td>\n      <td>-0.056843</td>\n      <td>-0.029515</td>\n      <td>0.358906</td>\n      <td>0.336873</td>\n      <td>-0.644819</td>\n      <td>-0.572268</td>\n      <td>-0.727720</td>\n      <td>-0.697862</td>\n      <td>-0.573309</td>\n      <td>-0.732708</td>\n      <td>-0.587498</td>\n      <td>-0.537909</td>\n      <td>-0.572071</td>\n      <td>-0.760827</td>\n      <td>-0.565077</td>\n      <td>-0.569148</td>\n      <td>-0.501580</td>\n      <td>-0.045894</td>\n      <td>-0.478671</td>\n      <td>-0.631316</td>\n      <td>-0.173600</td>\n      <td>-0.670007</td>\n      <td>-0.091948</td>\n      <td>-0.593847</td>\n      <td>-0.556878</td>\n      <td>-0.474824</td>\n      <td>-0.660880</td>\n      <td>-0.355750</td>\n      <td>-0.597264</td>\n      <td>-0.348168</td>\n      <td>-0.538036</td>\n      <td>-0.520658</td>\n      <td>-0.499096</td>\n      <td>0.965644</td>\n      <td>0.965169</td>\n      <td>1.000000</td>\n      <td>0.964271</td>\n      <td>0.966318</td>\n    </tr>\n    <tr>\n      <th>GKPositioning</th>\n      <td>-0.024020</td>\n      <td>0.115344</td>\n      <td>-0.016577</td>\n      <td>-0.050387</td>\n      <td>-0.026458</td>\n      <td>0.362008</td>\n      <td>0.341163</td>\n      <td>-0.645475</td>\n      <td>-0.573927</td>\n      <td>-0.725979</td>\n      <td>-0.697495</td>\n      <td>-0.574434</td>\n      <td>-0.734391</td>\n      <td>-0.590877</td>\n      <td>-0.540718</td>\n      <td>-0.572353</td>\n      <td>-0.761163</td>\n      <td>-0.565363</td>\n      <td>-0.568525</td>\n      <td>-0.501704</td>\n      <td>-0.035819</td>\n      <td>-0.478434</td>\n      <td>-0.633348</td>\n      <td>-0.167770</td>\n      <td>-0.669530</td>\n      <td>-0.086262</td>\n      <td>-0.595083</td>\n      <td>-0.554751</td>\n      <td>-0.470975</td>\n      <td>-0.662192</td>\n      <td>-0.357017</td>\n      <td>-0.600452</td>\n      <td>-0.344019</td>\n      <td>-0.535412</td>\n      <td>-0.517581</td>\n      <td>-0.495664</td>\n      <td>0.969816</td>\n      <td>0.969420</td>\n      <td>0.964271</td>\n      <td>1.000000</td>\n      <td>0.970079</td>\n    </tr>\n    <tr>\n      <th>GKReflexes</th>\n      <td>-0.024625</td>\n      <td>0.102066</td>\n      <td>-0.022430</td>\n      <td>-0.051285</td>\n      <td>-0.027211</td>\n      <td>0.362594</td>\n      <td>0.340019</td>\n      <td>-0.648011</td>\n      <td>-0.576336</td>\n      <td>-0.730731</td>\n      <td>-0.702814</td>\n      <td>-0.577288</td>\n      <td>-0.737654</td>\n      <td>-0.592644</td>\n      <td>-0.543333</td>\n      <td>-0.576941</td>\n      <td>-0.765924</td>\n      <td>-0.567027</td>\n      <td>-0.570517</td>\n      <td>-0.504113</td>\n      <td>-0.041116</td>\n      <td>-0.481105</td>\n      <td>-0.635971</td>\n      <td>-0.171295</td>\n      <td>-0.673544</td>\n      <td>-0.090339</td>\n      <td>-0.598392</td>\n      <td>-0.559176</td>\n      <td>-0.475956</td>\n      <td>-0.665543</td>\n      <td>-0.362782</td>\n      <td>-0.602818</td>\n      <td>-0.351831</td>\n      <td>-0.540356</td>\n      <td>-0.521492</td>\n      <td>-0.499555</td>\n      <td>0.973334</td>\n      <td>0.970251</td>\n      <td>0.966318</td>\n      <td>0.970079</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                     Wage       Age   Overall  Potential     Value    Height  \\\nWage             1.000000  0.143773  0.576150   0.488808  0.858080  0.019966   \nAge              0.143773  1.000000  0.452519  -0.253846  0.077141  0.081980   \nOverall          0.576150  0.452519  1.000000   0.660605  0.631570  0.038444   \nPotential        0.488808 -0.253846  0.660605   1.000000  0.579170 -0.009818   \nValue            0.858080  0.077141  0.631570   0.579170  1.000000  0.002879   \nHeight           0.019966  0.081980  0.038444  -0.009818  0.002879  1.000000   \nWeight           0.065594  0.229551  0.154119  -0.007530  0.046616  0.754278   \nCrossing         0.234813  0.130518  0.396615   0.249085  0.251745 -0.481792   \nFinishing        0.219319  0.069622  0.335101   0.245919  0.258606 -0.366690   \nHeadingAccuracy  0.190332  0.147051  0.342988   0.204485  0.186815  0.013868   \nShortPassing     0.296287  0.132018  0.499904   0.368916  0.326873 -0.357552   \nVolleys          0.259252  0.142423  0.392951   0.257102  0.290156 -0.345871   \nDribbling        0.238315  0.011541  0.374591   0.317479  0.273076 -0.486929   \nCurve            0.260829  0.143599  0.420967   0.281499  0.288518 -0.436651   \nFKAccuracy       0.238217  0.193484  0.398798   0.232954  0.267599 -0.400265   \nLongPassing      0.277910  0.180289  0.482774   0.322378  0.303158 -0.325218   \nBallControl      0.277971  0.085402  0.460365   0.355717  0.308977 -0.408868   \nAcceleration     0.127312 -0.154191  0.199887   0.238047  0.172122 -0.531553   \nSprintSpeed      0.132349 -0.146779  0.213534   0.239984  0.173931 -0.452834   \nAgility          0.157302 -0.017642  0.266740   0.225557  0.194676 -0.606220   \nReactions        0.477211  0.427635  0.812473   0.496197  0.519866 -0.017717   \nBalance          0.091619 -0.086890  0.108357   0.143630  0.115995 -0.762855   \nShotPower        0.259531  0.156358  0.442167   0.290572  0.282451 -0.286300   \nJumping          0.129796  0.172238  0.263994   0.114496  0.124827 -0.065480   \nStamina          0.179546  0.097799  0.365773   0.205499  0.212278 -0.280670   \nStrength         0.140250  0.323395  0.347049   0.082126  0.130114  0.519396   \nLongShots        0.251102  0.155028  0.422599   0.268951  0.281763 -0.377344   \nAggression       0.196484  0.263356  0.396327   0.173991  0.186601 -0.042109   \nInterceptions    0.159995  0.197649  0.324114   0.158222  0.143222 -0.049344   \nPositioning      0.228335  0.083854  0.359001   0.248104  0.260952 -0.430577   \nVision           0.314749  0.185797  0.497630   0.348328  0.356646 -0.360994   \nPenalties        0.224685  0.138605  0.342878   0.227321  0.241207 -0.335078   \nComposure        0.413919  0.379930  0.713420   0.434466  0.443918 -0.129665   \nMarking          0.148582  0.143118  0.289565   0.166234  0.136821 -0.072609   \nStandingTackle   0.129007  0.119987  0.255716   0.147085  0.111076 -0.057870   \nSlidingTackle    0.113800  0.103082  0.225634   0.132381  0.090444 -0.066151   \nGKDiving        -0.024234  0.099971 -0.024982  -0.051163 -0.027272  0.360583   \nGKHandling      -0.023797  0.105088 -0.024332  -0.052652 -0.027555  0.360914   \nGKKicking       -0.026884  0.103533 -0.028483  -0.056843 -0.029515  0.358906   \nGKPositioning   -0.024020  0.115344 -0.016577  -0.050387 -0.026458  0.362008   \nGKReflexes      -0.024625  0.102066 -0.022430  -0.051285 -0.027211  0.362594   \n\n                   Weight  Crossing  Finishing  HeadingAccuracy  ShortPassing  \\\nWage             0.065594  0.234813   0.219319         0.190332      0.296287   \nAge              0.229551  0.130518   0.069622         0.147051      0.132018   \nOverall          0.154119  0.396615   0.335101         0.342988      0.499904   \nPotential       -0.007530  0.249085   0.245919         0.204485      0.368916   \nValue            0.046616  0.251745   0.258606         0.186815      0.326873   \nHeight           0.754278 -0.481792  -0.366690         0.013868     -0.357552   \nWeight           1.000000 -0.389106  -0.289526         0.037412     -0.283021   \nCrossing        -0.389106  1.000000   0.661233         0.479991      0.813317   \nFinishing       -0.289526  0.661233   1.000000         0.483089      0.667675   \nHeadingAccuracy  0.037412  0.479991   0.483089         1.000000      0.650725   \nShortPassing    -0.283021  0.813317   0.667675         0.650725      1.000000   \nVolleys         -0.260315  0.695608   0.884473         0.515015      0.703631   \nDribbling       -0.408437  0.859764   0.826993         0.561127      0.847526   \nCurve           -0.342680  0.837136   0.762459         0.451824      0.779525   \nFKAccuracy      -0.301647  0.765394   0.701886         0.418870      0.741013   \nLongPassing     -0.255799  0.762133   0.522642         0.523571      0.898834   \nBallControl     -0.331395  0.844315   0.791308         0.667841      0.914923   \nAcceleration    -0.465779  0.675733   0.613557         0.349998      0.584460   \nSprintSpeed     -0.399840  0.653687   0.601499         0.398406      0.574006   \nAgility         -0.521236  0.704819   0.650450         0.283049      0.629232   \nReactions        0.082039  0.410975   0.351554         0.357295      0.515189   \nBalance         -0.645334  0.627755   0.533800         0.196558      0.554252   \nShotPower       -0.187543  0.712821   0.818319         0.622334      0.778944   \nJumping          0.009483  0.167838   0.126284         0.402972      0.241976   \nStamina         -0.216783  0.679565   0.520971         0.644132      0.727659   \nStrength         0.596124  0.007803   0.022010         0.505381      0.179505   \nLongShots       -0.275134  0.746821   0.879485         0.516290      0.765440   \nAggression       0.031942  0.485786   0.259058         0.701324      0.625008   \nInterceptions   -0.024375  0.436860  -0.006059         0.556068      0.552991   \nPositioning     -0.346132  0.787018   0.890732         0.543404      0.762505   \nVision          -0.277647  0.692419   0.702071         0.296463      0.724392   \nPenalties       -0.249198  0.653712   0.840845         0.562427      0.685726   \nComposure       -0.032196  0.587027   0.543090         0.525061      0.701340   \nMarking         -0.047815  0.452757   0.039418         0.590872      0.569873   \nStandingTackle  -0.045426  0.437817  -0.018439         0.568036      0.550521   \nSlidingTackle   -0.054921  0.418693  -0.057634         0.540439      0.518118   \nGKDiving         0.338844 -0.648413  -0.577767        -0.732289     -0.703716   \nGKHandling       0.338018 -0.645549  -0.576122        -0.731461     -0.701893   \nGKKicking        0.336873 -0.644819  -0.572268        -0.727720     -0.697862   \nGKPositioning    0.341163 -0.645475  -0.573927        -0.725979     -0.697495   \nGKReflexes       0.340019 -0.648011  -0.576336        -0.730731     -0.702814   \n\n                  Volleys  Dribbling     Curve  FKAccuracy  LongPassing  \\\nWage             0.259252   0.238315  0.260829    0.238217     0.277910   \nAge              0.142423   0.011541  0.143599    0.193484     0.180289   \nOverall          0.392951   0.374591  0.420967    0.398798     0.482774   \nPotential        0.257102   0.317479  0.281499    0.232954     0.322378   \nValue            0.290156   0.273076  0.288518    0.267599     0.303158   \nHeight          -0.345871  -0.486929 -0.436651   -0.400265    -0.325218   \nWeight          -0.260315  -0.408437 -0.342680   -0.301647    -0.255799   \nCrossing         0.695608   0.859764  0.837136    0.765394     0.762133   \nFinishing        0.884473   0.826993  0.762459    0.701886     0.522642   \nHeadingAccuracy  0.515015   0.561127  0.451824    0.418870     0.523571   \nShortPassing     0.703631   0.847526  0.779525    0.741013     0.898834   \nVolleys          1.000000   0.812841  0.809694    0.753334     0.579671   \nDribbling        0.812841   1.000000  0.845635    0.757763     0.729923   \nCurve            0.809694   0.845635  1.000000    0.863569     0.717701   \nFKAccuracy       0.753334   0.757763  0.863569    1.000000     0.709906   \nLongPassing      0.579671   0.729923  0.717701    0.709906     1.000000   \nBallControl      0.798123   0.940201  0.832536    0.763696     0.795900   \nAcceleration     0.580793   0.754575  0.616131    0.509074     0.462958   \nSprintSpeed      0.566128   0.733831  0.588593    0.478839     0.447707   \nAgility          0.631961   0.770942  0.688305    0.598157     0.540945   \nReactions        0.411213   0.395887  0.431137    0.415244     0.487690   \nBalance          0.524656   0.672702  0.597053    0.532279     0.483199   \nShotPower        0.835336   0.809628  0.795924    0.758717     0.681522   \nJumping          0.154566   0.178042  0.143184    0.114067     0.194380   \nStamina          0.536922   0.694809  0.599638    0.546884     0.647276   \nStrength         0.060482   0.007214 -0.000667    0.014608     0.154642   \nLongShots        0.870139   0.846276  0.837311    0.805604     0.674899   \nAggression       0.344829   0.456431  0.413465    0.409614     0.602342   \nInterceptions    0.102011   0.308926  0.286178    0.306554     0.604220   \nPositioning      0.850916   0.898585  0.814364    0.733721     0.623370   \nVision           0.704398   0.736619  0.749207    0.722136     0.708672   \nPenalties        0.833141   0.775196  0.756885    0.739379     0.555080   \nComposure        0.603296   0.610364  0.624992    0.593813     0.659748   \nMarking          0.135439   0.349305  0.303079    0.310198     0.595358   \nStandingTackle   0.086371   0.313817  0.274199    0.290496     0.594717   \nSlidingTackle    0.048595   0.286320  0.245469    0.259141     0.569450   \nGKDiving        -0.579121  -0.737743 -0.593628   -0.544894    -0.577847   \nGKHandling      -0.577077  -0.736281 -0.590646   -0.541930    -0.575926   \nGKKicking       -0.573309  -0.732708 -0.587498   -0.537909    -0.572071   \nGKPositioning   -0.574434  -0.734391 -0.590877   -0.540718    -0.572353   \nGKReflexes      -0.577288  -0.737654 -0.592644   -0.543333    -0.576941   \n\n                 BallControl  Acceleration  SprintSpeed   Agility  Reactions  \\\nWage                0.277971      0.127312     0.132349  0.157302   0.477211   \nAge                 0.085402     -0.154191    -0.146779 -0.017642   0.427635   \nOverall             0.460365      0.199887     0.213534  0.266740   0.812473   \nPotential           0.355717      0.238047     0.239984  0.225557   0.496197   \nValue               0.308977      0.172122     0.173931  0.194676   0.519866   \nHeight             -0.408868     -0.531553    -0.452834 -0.606220  -0.017717   \nWeight             -0.331395     -0.465779    -0.399840 -0.521236   0.082039   \nCrossing            0.844315      0.675733     0.653687  0.704819   0.410975   \nFinishing           0.791308      0.613557     0.601499  0.650450   0.351554   \nHeadingAccuracy     0.667841      0.349998     0.398406  0.283049   0.357295   \nShortPassing        0.914923      0.584460     0.574006  0.629232   0.515189   \nVolleys             0.798123      0.580793     0.566128  0.631961   0.411213   \nDribbling           0.940201      0.754575     0.733831  0.770942   0.395887   \nCurve               0.832536      0.616131     0.588593  0.688305   0.431137   \nFKAccuracy          0.763696      0.509074     0.478839  0.598157   0.415244   \nLongPassing         0.795900      0.462958     0.447707  0.540945   0.487690   \nBallControl         1.000000      0.687606     0.676341  0.715133   0.473259   \nAcceleration        0.687606      1.000000     0.925596  0.819908   0.247078   \nSprintSpeed         0.676341      0.925596     1.000000  0.775135   0.251993   \nAgility             0.715133      0.819908     0.775135  1.000000   0.326891   \nReactions           0.473259      0.247078     0.251993  0.326891   1.000000   \nBalance             0.616656      0.725896     0.661363  0.782358   0.214262   \nShotPower           0.836555      0.555020     0.560175  0.587873   0.445834   \nJumping             0.234598      0.263338     0.279875  0.262609   0.323820   \nStamina             0.738188      0.624012     0.636321  0.586842   0.409812   \nStrength            0.130016     -0.101243    -0.021514 -0.165643   0.348889   \nLongShots           0.838438      0.588569     0.570475  0.651534   0.437301   \nAggression          0.564785      0.276687     0.304117  0.267279   0.431116   \nInterceptions       0.431224      0.172210     0.183772  0.159105   0.356473   \nPositioning         0.866036      0.688217     0.671696  0.713531   0.406754   \nVision              0.727291      0.483549     0.453312  0.613975   0.530670   \nPenalties           0.776198      0.547733     0.536504  0.579791   0.376308   \nComposure           0.688709      0.383688     0.388814  0.463948   0.712632   \nMarking             0.465785      0.215240     0.231942  0.187999   0.307710   \nStandingTackle      0.430032      0.181819     0.196553  0.149024   0.278040   \nSlidingTackle       0.397209      0.175950     0.189906  0.136166   0.251614   \nGKDiving           -0.766158     -0.566592    -0.570151 -0.502715  -0.043725   \nGKHandling         -0.764445     -0.567988    -0.571603 -0.503145  -0.042483   \nGKKicking          -0.760827     -0.565077    -0.569148 -0.501580  -0.045894   \nGKPositioning      -0.761163     -0.565363    -0.568525 -0.501704  -0.035819   \nGKReflexes         -0.765924     -0.567027    -0.570517 -0.504113  -0.041116   \n\n                  Balance  ShotPower   Jumping   Stamina  Strength  LongShots  \\\nWage             0.091619   0.259531  0.129796  0.179546  0.140250   0.251102   \nAge             -0.086890   0.156358  0.172238  0.097799  0.323395   0.155028   \nOverall          0.108357   0.442167  0.263994  0.365773  0.347049   0.422599   \nPotential        0.143630   0.290572  0.114496  0.205499  0.082126   0.268951   \nValue            0.115995   0.282451  0.124827  0.212278  0.130114   0.281763   \nHeight          -0.762855  -0.286300 -0.065480 -0.280670  0.519396  -0.377344   \nWeight          -0.645334  -0.187543  0.009483 -0.216783  0.596124  -0.275134   \nCrossing         0.627755   0.712821  0.167838  0.679565  0.007803   0.746821   \nFinishing        0.533800   0.818319  0.126284  0.520971  0.022010   0.879485   \nHeadingAccuracy  0.196558   0.622334  0.402972  0.644132  0.505381   0.516290   \nShortPassing     0.554252   0.778944  0.241976  0.727659  0.179505   0.765440   \nVolleys          0.524656   0.835336  0.154566  0.536922  0.060482   0.870139   \nDribbling        0.672702   0.809628  0.178042  0.694809  0.007214   0.846276   \nCurve            0.597053   0.795924  0.143184  0.599638 -0.000667   0.837311   \nFKAccuracy       0.532279   0.758717  0.114067  0.546884  0.014608   0.805604   \nLongPassing      0.483199   0.681522  0.194380  0.647276  0.154642   0.674899   \nBallControl      0.616656   0.836555  0.234598  0.738188  0.130016   0.838438   \nAcceleration     0.725896   0.555020  0.263338  0.624012 -0.101243   0.588569   \nSprintSpeed      0.661363   0.560175  0.279875  0.636321 -0.021514   0.570475   \nAgility          0.782358   0.587873  0.262609  0.586842 -0.165643   0.651534   \nReactions        0.214262   0.445834  0.323820  0.409812  0.348889   0.437301   \nBalance          1.000000   0.477379  0.241015  0.498322 -0.308757   0.543796   \nShotPower        0.477379   1.000000  0.221465  0.628771  0.204837   0.890791   \nJumping          0.241015   0.221465  1.000000  0.382232  0.332519   0.164618   \nStamina          0.498322   0.628771  0.382232  1.000000  0.301892   0.604667   \nStrength        -0.308757   0.204837  0.332519  0.301892  1.000000   0.080933   \nLongShots        0.543796   0.890791  0.164618  0.604667  0.080933   1.000000   \nAggression       0.215509   0.507299  0.398800  0.657313  0.493954   0.406138   \nInterceptions    0.172065   0.279940  0.307693  0.584096  0.372525   0.206204   \nPositioning      0.605203   0.813162  0.174225  0.648854  0.041579   0.863438   \nVision           0.513511   0.689388  0.109636  0.492403  0.005181   0.757464   \nPenalties        0.500145   0.800470  0.169999  0.530534  0.092813   0.815938   \nComposure        0.351383   0.647903  0.305408  0.548025  0.330239   0.623534   \nMarking          0.200620   0.312568  0.300401  0.596170  0.351767   0.228920   \nStandingTackle   0.174597   0.271286  0.279959  0.577118  0.348424   0.185134   \nSlidingTackle    0.172708   0.234967  0.278948  0.551795  0.321084   0.146129   \nGKDiving        -0.479692  -0.636334 -0.171809 -0.675158 -0.093647  -0.600338   \nGKHandling      -0.480854  -0.636189 -0.172220 -0.672093 -0.091811  -0.598725   \nGKKicking       -0.478671  -0.631316 -0.173600 -0.670007 -0.091948  -0.593847   \nGKPositioning   -0.478434  -0.633348 -0.167770 -0.669530 -0.086262  -0.595083   \nGKReflexes      -0.481105  -0.635971 -0.171295 -0.673544 -0.090339  -0.598392   \n\n                 Aggression  Interceptions  Positioning    Vision  Penalties  \\\nWage               0.196484       0.159995     0.228335  0.314749   0.224685   \nAge                0.263356       0.197649     0.083854  0.185797   0.138605   \nOverall            0.396327       0.324114     0.359001  0.497630   0.342878   \nPotential          0.173991       0.158222     0.248104  0.348328   0.227321   \nValue              0.186601       0.143222     0.260952  0.356646   0.241207   \nHeight            -0.042109      -0.049344    -0.430577 -0.360994  -0.335078   \nWeight             0.031942      -0.024375    -0.346132 -0.277647  -0.249198   \nCrossing           0.485786       0.436860     0.787018  0.692419   0.653712   \nFinishing          0.259058      -0.006059     0.890732  0.702071   0.840845   \nHeadingAccuracy    0.701324       0.556068     0.543404  0.296463   0.562427   \nShortPassing       0.625008       0.552991     0.762505  0.724392   0.685726   \nVolleys            0.344829       0.102011     0.850916  0.704398   0.833141   \nDribbling          0.456431       0.308926     0.898585  0.736619   0.775196   \nCurve              0.413465       0.286178     0.814364  0.749207   0.756885   \nFKAccuracy         0.409614       0.306554     0.733721  0.722136   0.739379   \nLongPassing        0.602342       0.604220     0.623370  0.708672   0.555080   \nBallControl        0.564785       0.431224     0.866036  0.727291   0.776198   \nAcceleration       0.276687       0.172210     0.688217  0.483549   0.547733   \nSprintSpeed        0.304117       0.183772     0.671696  0.453312   0.536504   \nAgility            0.267279       0.159105     0.713531  0.613975   0.579791   \nReactions          0.431116       0.356473     0.406754  0.530670   0.376308   \nBalance            0.215509       0.172065     0.605203  0.513511   0.500145   \nShotPower          0.507299       0.279940     0.813162  0.689388   0.800470   \nJumping            0.398800       0.307693     0.174225  0.109636   0.169999   \nStamina            0.657313       0.584096     0.648854  0.492403   0.530534   \nStrength           0.493954       0.372525     0.041579  0.005181   0.092813   \nLongShots          0.406138       0.206204     0.863438  0.757464   0.815938   \nAggression         1.000000       0.755429     0.396618  0.322492   0.354806   \nInterceptions      0.755429       1.000000     0.184507  0.201596   0.126989   \nPositioning        0.396618       0.184507     1.000000  0.739196   0.805887   \nVision             0.322492       0.201596     0.739196  1.000000   0.644031   \nPenalties          0.354806       0.126989     0.805887  0.644031   1.000000   \nComposure          0.535170       0.412375     0.590992  0.652474   0.567862   \nMarking            0.729088       0.890207     0.216638  0.196451   0.169029   \nStandingTackle     0.747817       0.942202     0.171454  0.165261   0.117891   \nSlidingTackle      0.724999       0.929105     0.137384  0.131883   0.082436   \nGKDiving          -0.559896      -0.475640    -0.666150 -0.363215  -0.603583   \nGKHandling        -0.559764      -0.476089    -0.664461 -0.359291  -0.602283   \nGKKicking         -0.556878      -0.474824    -0.660880 -0.355750  -0.597264   \nGKPositioning     -0.554751      -0.470975    -0.662192 -0.357017  -0.600452   \nGKReflexes        -0.559176      -0.475956    -0.665543 -0.362782  -0.602818   \n\n                 Composure   Marking  StandingTackle  SlidingTackle  GKDiving  \\\nWage              0.413919  0.148582        0.129007       0.113800 -0.024234   \nAge               0.379930  0.143118        0.119987       0.103082  0.099971   \nOverall           0.713420  0.289565        0.255716       0.225634 -0.024982   \nPotential         0.434466  0.166234        0.147085       0.132381 -0.051163   \nValue             0.443918  0.136821        0.111076       0.090444 -0.027272   \nHeight           -0.129665 -0.072609       -0.057870      -0.066151  0.360583   \nWeight           -0.032196 -0.047815       -0.045426      -0.054921  0.338844   \nCrossing          0.587027  0.452757        0.437817       0.418693 -0.648413   \nFinishing         0.543090  0.039418       -0.018439      -0.057634 -0.577767   \nHeadingAccuracy   0.525061  0.590872        0.568036       0.540439 -0.732289   \nShortPassing      0.701340  0.569873        0.550521       0.518118 -0.703716   \nVolleys           0.603296  0.135439        0.086371       0.048595 -0.579121   \nDribbling         0.610364  0.349305        0.313817       0.286320 -0.737743   \nCurve             0.624992  0.303079        0.274199       0.245469 -0.593628   \nFKAccuracy        0.593813  0.310198        0.290496       0.259141 -0.544894   \nLongPassing       0.659748  0.595358        0.594717       0.569450 -0.577847   \nBallControl       0.688709  0.465785        0.430032       0.397209 -0.766158   \nAcceleration      0.383688  0.215240        0.181819       0.175950 -0.566592   \nSprintSpeed       0.388814  0.231942        0.196553       0.189906 -0.570151   \nAgility           0.463948  0.187999        0.149024       0.136166 -0.502715   \nReactions         0.712632  0.307710        0.278040       0.251614 -0.043725   \nBalance           0.351383  0.200620        0.174597       0.172708 -0.479692   \nShotPower         0.647903  0.312568        0.271286       0.234967 -0.636334   \nJumping           0.305408  0.300401        0.279959       0.278948 -0.171809   \nStamina           0.548025  0.596170        0.577118       0.551795 -0.675158   \nStrength          0.330239  0.351767        0.348424       0.321084 -0.093647   \nLongShots         0.623534  0.228920        0.185134       0.146129 -0.600338   \nAggression        0.535170  0.729088        0.747817       0.724999 -0.559896   \nInterceptions     0.412375  0.890207        0.942202       0.929105 -0.475640   \nPositioning       0.590992  0.216638        0.171454       0.137384 -0.666150   \nVision            0.652474  0.196451        0.165261       0.131883 -0.363215   \nPenalties         0.567862  0.169029        0.117891       0.082436 -0.603583   \nComposure         1.000000  0.401127        0.367515       0.333513 -0.352608   \nMarking           0.401127  1.000000        0.907658       0.897192 -0.539977   \nStandingTackle    0.367515  0.907658        1.000000       0.974926 -0.521123   \nSlidingTackle     0.333513  0.897192        0.974926       1.000000 -0.499579   \nGKDiving         -0.352608 -0.539977       -0.521123      -0.499579  1.000000   \nGKHandling       -0.349402 -0.540849       -0.521956      -0.500562  0.970220   \nGKKicking        -0.348168 -0.538036       -0.520658      -0.499096  0.965644   \nGKPositioning    -0.344019 -0.535412       -0.517581      -0.495664  0.969816   \nGKReflexes       -0.351831 -0.540356       -0.521492      -0.499555  0.973334   \n\n                 GKHandling  GKKicking  GKPositioning  GKReflexes  \nWage              -0.023797  -0.026884      -0.024020   -0.024625  \nAge                0.105088   0.103533       0.115344    0.102066  \nOverall           -0.024332  -0.028483      -0.016577   -0.022430  \nPotential         -0.052652  -0.056843      -0.050387   -0.051285  \nValue             -0.027555  -0.029515      -0.026458   -0.027211  \nHeight             0.360914   0.358906       0.362008    0.362594  \nWeight             0.338018   0.336873       0.341163    0.340019  \nCrossing          -0.645549  -0.644819      -0.645475   -0.648011  \nFinishing         -0.576122  -0.572268      -0.573927   -0.576336  \nHeadingAccuracy   -0.731461  -0.727720      -0.725979   -0.730731  \nShortPassing      -0.701893  -0.697862      -0.697495   -0.702814  \nVolleys           -0.577077  -0.573309      -0.574434   -0.577288  \nDribbling         -0.736281  -0.732708      -0.734391   -0.737654  \nCurve             -0.590646  -0.587498      -0.590877   -0.592644  \nFKAccuracy        -0.541930  -0.537909      -0.540718   -0.543333  \nLongPassing       -0.575926  -0.572071      -0.572353   -0.576941  \nBallControl       -0.764445  -0.760827      -0.761163   -0.765924  \nAcceleration      -0.567988  -0.565077      -0.565363   -0.567027  \nSprintSpeed       -0.571603  -0.569148      -0.568525   -0.570517  \nAgility           -0.503145  -0.501580      -0.501704   -0.504113  \nReactions         -0.042483  -0.045894      -0.035819   -0.041116  \nBalance           -0.480854  -0.478671      -0.478434   -0.481105  \nShotPower         -0.636189  -0.631316      -0.633348   -0.635971  \nJumping           -0.172220  -0.173600      -0.167770   -0.171295  \nStamina           -0.672093  -0.670007      -0.669530   -0.673544  \nStrength          -0.091811  -0.091948      -0.086262   -0.090339  \nLongShots         -0.598725  -0.593847      -0.595083   -0.598392  \nAggression        -0.559764  -0.556878      -0.554751   -0.559176  \nInterceptions     -0.476089  -0.474824      -0.470975   -0.475956  \nPositioning       -0.664461  -0.660880      -0.662192   -0.665543  \nVision            -0.359291  -0.355750      -0.357017   -0.362782  \nPenalties         -0.602283  -0.597264      -0.600452   -0.602818  \nComposure         -0.349402  -0.348168      -0.344019   -0.351831  \nMarking           -0.540849  -0.538036      -0.535412   -0.540356  \nStandingTackle    -0.521956  -0.520658      -0.517581   -0.521492  \nSlidingTackle     -0.500562  -0.499096      -0.495664   -0.499555  \nGKDiving           0.970220   0.965644       0.969816    0.973334  \nGKHandling         1.000000   0.965169       0.969420    0.970251  \nGKKicking          0.965169   1.000000       0.964271    0.966318  \nGKPositioning      0.969420   0.964271       1.000000    0.970079  \nGKReflexes         0.970251   0.966318       0.970079    1.000000  "}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Assessing Attributes Through Correlation\nIn the correlation matrix above, one may observe that the following attributes has a high correlation  (greater or equal to 0.90). <br>\nCorr(Dribbling, BallControl) = 0.94 <br>\nCorr(Acceleration, SprintSpeed) = 0.93 <br>\nCorr(StandingTackle, Interception) = 0.94, Corr(SlidingTackle, Interception) = 0.93, Corr(Marking, StandingTackle) = 0.90, Corr(Marking, SlidingTackle) = 0.90, Corr(Marking, Interception) = 0.89, Corr(StandingTackle, SlidingTackle) = 0.97 <br>\nAnd the correlations between \"GKDiving\", \"GKHandling\", \"GKKicking\", \"GKPositioning\", \"GKReflexes\" are all above 0.90. <br>\n<br>\nFor attributes that has high correlations, only one of them are remained. So the following attributes are removed. <br>\n\"BallControl\", \"SprintSpeed\", \"SlidingTackle\", \"Interception\", \"Marking\", \"GKHandling\", \"GKKicking\", \"GKPositioning\", \"GKReflexes\". "}, {"metadata": {}, "cell_type": "code", "source": "attributes = ['Age', 'Overall', 'Potential', 'Value', 'Height', 'Weight', 'Crossing', 'Finishing', \n              'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', \n              'Acceleration', 'Agility', 'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength', \n              'LongShots', 'Aggression', 'Positioning', 'Vision', 'Penalties', 'Composure', 'StandingTackle', 'GKDiving']", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_1[attributes].shape", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "(17966, 31)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Training/Test Split and Rescaling\nAfter removing correlated attributes, it is necessary to divide the dataset into different set. The point is to train the model on a dataset and test its performance on an unseen dataset. <br> \nP.S. Note that some commands in Python enables us to access cross-validation score (like LASSO from sklearn). Those metrics scores are helpful as well. But for most of the time, the score on the test set is a better estimate of the model's performance on the unseen data. <br>\n<br>\nThere're two ways to re-scale the data set. Standardization and normalization. Standardization rescales the data so that it has a mean of 0 and standard deviation 1; normalization rescale the attribute to range between 0 and 1. In python, one may use `StandardScaler()` to standardize the data and `MinMaxScaler()` to normalize the data. It's difficult to tell which method is better. This project will stick with standardization. <br>"}, {"metadata": {}, "cell_type": "code", "source": "x_tr, x_test, y_tr, y_test = train_test_split(df_1[attributes], df_1['Wage'], test_size = 0.33)", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# two ways to scale the data\nscaler = StandardScaler()\nx_tr_std = scaler.fit_transform(x_tr)\nx_test_std = scaler.transform(x_test)\ny_tr_std = scaler.fit_transform(np.array(y_tr).reshape(-1, 1))\ny_test_std = scaler.transform(np.array(y_test).reshape(-1, 1))", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Introducing the metric measuring the performance of the model\nThe performance of model on unseen data is evaluated on the Mean Squared Error (MSE).  It measures how different the predicted value is differnt from the actual value. The formula is given below. Mathematically, it is the mean of the squared error. A detailed discusssion of MSE is in the model evaluation chapter. <br>\n$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2$ <br>\nP.S. Some might argue that AIC (Akaike Information Criterion) is a better criterion for linear regression model. However, it is very difficult to get the number of estimated parameters in some models (like random forest). We have to use a consistent metric to evalute the model performance. The reason for selecting model algorithms are explained in the next section.  "}, {"metadata": {}, "cell_type": "markdown", "source": "### 2 Model Construction\nThe following models are considered: <br>\n\n-----\n1. Linear Regression & LASSO<br>\n2. Random Forest Regression (or Regression Forest) <br>\n3. Neural Network\n\n------\n\nThe reason for selecting linear regression is straightforward. If the data is linearly separable, then it is effective to predict wage using linear regression. <br>\nHowever, if the data is not linear separable, random forest regression is more effective in capturing this non-linearity. Additionally, it is one of the machine learning models. <br>\nNeural network is one of the deep learning models that is strong at prediction. However, it is not very interpretable. "}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Linear Regression & LASSO\nThe project begin by implementing a basic linear regression model on the training set. "}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlmmodel_1 = lm.fit(x_tr_std, y_tr_std)", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "coefficients = lmmodel_1.coef_\nintercept = lmmodel_1.intercept_\nprint('Coefficients:', coefficients)\nprint('Intercept:', intercept)", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "Coefficients: [[ 0.12054628 -0.07776646  0.0564994   0.87766307  0.02947106  0.01557953\n   0.05027061 -0.03386721  0.03004789 -0.00397389  0.00931532  0.01476425\n   0.00853067 -0.04595853 -0.01024697 -0.00931594  0.00299173 -0.01880163\n   0.02874845  0.01996634  0.00861733 -0.05090852 -0.02381851  0.0083552\n   0.00752086  0.0122537  -0.00570033  0.02684008 -0.00150002  0.03922478\n   0.03234762]]\nIntercept: [2.98898262e-17]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_pred1 = lmmodel_1.predict(x_test_std)", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import mean_squared_error\nprint('MSE:', mean_squared_error(y_test_std, y_test_pred1))", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "MSE: 0.2403890390313032\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "LASSO is a regression analysis that performs variable selection and regularization. Compared with the least square linear models, LASSO penalizes non-zero coefficients. "}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV", "execution_count": 22, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "lasso = Lasso()\nparam = {'alpha': [1e-20, 1e-10, 1e-3, 1e-1, 1, 5, 20]}\nlasso_regressor = GridSearchCV(lasso, param, scoring = 'neg_mean_squared_error', cv = 5)\nlasso_regressor.fit(x_tr_std, y_tr_std)\nprint('Best alpha:', lasso_regressor.best_params_)", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "Best alpha: {'alpha': 1e-10}\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "lasso_coeff = Lasso(alpha = 1e-10)\nlasso_coeff = lasso_coeff.fit(x_tr_std, y_tr_std)\ny_test_pred_lasso_coeff = lasso_coeff.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_pred_lasso_coeff))\nprint('Coefficients', lasso_coeff.coef_, 'Intercept:', lasso_coeff.intercept_)", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "MSE: 0.24038903897495603\nCoefficients [ 0.12054628 -0.07776645  0.0564994   0.87766307  0.02947106  0.01557953\n  0.05027061 -0.03386721  0.03004789 -0.00397388  0.00931532  0.01476425\n  0.00853067 -0.04595853 -0.01024697 -0.00931593  0.00299173 -0.01880163\n  0.02874845  0.01996633  0.00861733 -0.05090852 -0.02381851  0.00835519\n  0.00752086  0.0122537  -0.00570033  0.02684008 -0.00150002  0.03922478\n  0.03234762] Intercept: [2.98898267e-17]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The MSE value of LASSO regularization is slightly smaller, so the mean of square error is smaller in LASSO. We prefer the LASSO regularization. "}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Random Forest\nRandom Forest is one of the machine learning methods. It is based on the idea of decision trees, but it's more effective than decision tree as a result of bagging. In case of random forest, we will not use GridSearchCV, which takes too long to complete (it's exhaustive searching combinations). Hence I will focus on running models on the training set first and evaluate the best hyperparameter using scores on the cross validation set. Then I will use the best hyperparameter to train the \"tr\" set and obtain the score on the test set. The best score is the smallest MSE. "}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.ensemble import RandomForestRegressor\nrf_1 = RandomForestRegressor(n_estimators = 100, random_state = 0)\nrf_1 = rf_1.fit(x_tr_std, y_tr_std)\ny_test_rfpred1 = rf_1.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred1))", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "MSE: 0.2132910756302537\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_2 = RandomForestRegressor(n_estimators = 100, max_depth = 10, max_features = 'log2', min_samples_split = 3, min_samples_leaf = 2, random_state = 0)\nrf_2 = rf_2.fit(x_tr_std, y_tr_std)\ny_test_rfpred2 = rf_2.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred2))", "execution_count": 31, "outputs": [{"output_type": "stream", "text": "MSE: 0.21245407062496136\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_3 = RandomForestRegressor(n_estimators = 1000, max_depth = 10, max_features = 'log2', random_state = 0)\nrf_3 = rf_3.fit(x_tr_std, y_tr_std)\ny_test_rfpred3 = rf_3.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred3))", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "MSE: 0.2102413359872055\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_4 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 'log2', random_state = 0)\nrf_4 = rf_4.fit(x_tr_std, y_tr_std)\ny_test_rfpred4 = rf_4.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred4))", "execution_count": 33, "outputs": [{"output_type": "stream", "text": "MSE: 0.20991229817069135\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_5 = RandomForestRegressor(n_estimators = 100, max_depth = 30, max_features = 'log2', random_state = 0)\nrf_5 = rf_5.fit(x_tr_std, y_tr_std)\ny_test_rfpred5 = rf_5.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred5))", "execution_count": 34, "outputs": [{"output_type": "stream", "text": "MSE: 0.21097812634303278\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_6 = RandomForestRegressor(n_estimators = 100, max_depth = 40, max_features = 'log2', random_state = 0)\nrf_6 = rf_6.fit(x_tr_std, y_tr_std)\ny_test_rfpred6 = rf_6.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred6))", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "MSE: 0.2122553073700639\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_7 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 20, random_state = 0)\nrf_7 = rf_7.fit(x_tr_std, y_tr_std)\ny_test_rfpred7 = rf_7.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred7))", "execution_count": 36, "outputs": [{"output_type": "stream", "text": "MSE: 0.20629759008702017\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_8 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 30, random_state = 0)\nrf_8 = rf_8.fit(x_tr_std, y_tr_std)\ny_test_rfpred8 = rf_8.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred8))", "execution_count": 37, "outputs": [{"output_type": "stream", "text": "MSE: 0.2101805854612528\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_9 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 31, random_state = 0)\nrf_9 = rf_9.fit(x_tr_std, y_tr_std)\ny_test_rfpred9 = rf_9.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred9))", "execution_count": 39, "outputs": [{"output_type": "stream", "text": "MSE: 0.2129534757725675\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rf_10 = RandomForestRegressor(n_estimators = 500, max_depth = 20, max_features = 31, random_state = 0)\nrf_10 = rf_10.fit(x_tr_std, y_tr_std)\ny_test_rfpred10 = rf_10.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_rfpred10))", "execution_count": 40, "outputs": [{"output_type": "stream", "text": "MSE: 0.20874740222698737\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Based on MSE, we use the following hyperparameters. \n\n-----\nn_estimators (number of trees in the forest) = 100 <br>\nmax_depth (maximum depth of the tree) = 20 <br>\nmax_features (the number of features to consider when looking for the best split) = 20 <br>\n\n------\n\nThe MSE score is approximately 0.206. This score is better than the score of LASSO. So prediction of random forest regression is better than linear regression with LASSO regularization. "}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Neural Network\nNeural network is an effective deep learning method. Compared with the two previous models (regression, random forest), neural network is more difficult to interpret. The following codes examines this model. "}, {"metadata": {}, "cell_type": "code", "source": "from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout", "execution_count": 42, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nnmodel = Sequential()\nnnmodel.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel.add(Dense(1, kernel_initializer = 'normal'))\nnnmodel.compile(loss = 'mean_squared_error', optimizer = 'adam')", "execution_count": 46, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nnmodel.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)", "execution_count": 47, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From /opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n", "name": "stderr"}, {"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.4908\nEpoch 2/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.2688\nEpoch 3/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.2365\nEpoch 4/10\n12037/12037 [==============================] - 36s 3ms/step - loss: 0.2302\nEpoch 5/10\n12037/12037 [==============================] - 34s 3ms/step - loss: 0.2204\nEpoch 6/10\n12037/12037 [==============================] - 36s 3ms/step - loss: 0.2200\nEpoch 7/10\n12037/12037 [==============================] - 36s 3ms/step - loss: 0.2135\nEpoch 8/10\n12037/12037 [==============================] - 40s 3ms/step - loss: 0.2106\nEpoch 9/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.2092\nEpoch 10/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.2080\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 47, "data": {"text/plain": "<keras.callbacks.History at 0x7fc6e3198a58>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn1 = nnmodel.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn1.reshape(5929,)))", "execution_count": 48, "outputs": [{"output_type": "stream", "text": "MSE: 0.21711179263820113\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel11 = Sequential()\nnnmodel11.add(Dense(40, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel11.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel11.compile(loss = 'mean_squared_error', optimizer = 'adam')\nnnmodel11.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)", "execution_count": 49, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 21s 2ms/step - loss: 0.5704\nEpoch 2/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.2939\nEpoch 3/10\n12037/12037 [==============================] - 20s 2ms/step - loss: 0.2491\nEpoch 4/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.2332\nEpoch 5/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.2276\nEpoch 6/10\n12037/12037 [==============================] - 18s 2ms/step - loss: 0.2190\nEpoch 7/10\n12037/12037 [==============================] - 20s 2ms/step - loss: 0.2162\nEpoch 8/10\n12037/12037 [==============================] - 17s 1ms/step - loss: 0.2150\nEpoch 9/10\n12037/12037 [==============================] - 20s 2ms/step - loss: 0.2111\nEpoch 10/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.2110\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 49, "data": {"text/plain": "<keras.callbacks.History at 0x7fc6c8350d68>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn11 = nnmodel11.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn11.reshape(5929,)))", "execution_count": 50, "outputs": [{"output_type": "stream", "text": "MSE: 0.22494578974951424\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel12 = Sequential()\nnnmodel12.add(Dense(20, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel12.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel12.compile(loss = 'mean_squared_error', optimizer = 'adam')\nnnmodel12.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)", "execution_count": 51, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 18s 2ms/step - loss: 0.5730\nEpoch 2/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.3130\nEpoch 3/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.2616\nEpoch 4/10\n12037/12037 [==============================] - 17s 1ms/step - loss: 0.2432\nEpoch 5/10\n12037/12037 [==============================] - 17s 1ms/step - loss: 0.2324\nEpoch 6/10\n12037/12037 [==============================] - 21s 2ms/step - loss: 0.2304\nEpoch 7/10\n12037/12037 [==============================] - 18s 1ms/step - loss: 0.2248\nEpoch 8/10\n12037/12037 [==============================] - 17s 1ms/step - loss: 0.2236\nEpoch 9/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.2211\nEpoch 10/10\n12037/12037 [==============================] - 20s 2ms/step - loss: 0.2239\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 51, "data": {"text/plain": "<keras.callbacks.History at 0x7fc6c834ecf8>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn12 = nnmodel12.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn12.reshape(5929,)))", "execution_count": 52, "outputs": [{"output_type": "stream", "text": "MSE: 0.22823854243098654\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel2 = Sequential()\nnnmodel2.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel2.add(Dropout(0.4))\nnnmodel2.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel2.add(Dropout(0.4))\nnnmodel2.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel2.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])", "execution_count": 54, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nnmodel2.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)", "execution_count": 55, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 77s 6ms/step - loss: 0.6855 - mean_squared_error: 0.6855 3s - l\nEpoch 2/10\n12037/12037 [==============================] - 78s 6ms/step - loss: 0.4747 - mean_squared_error: 0.4747\nEpoch 3/10\n12037/12037 [==============================] - 75s 6ms/step - loss: 0.3949 - mean_squared_error: 0.3949\nEpoch 4/10\n12037/12037 [==============================] - 73s 6ms/step - loss: 0.3273 - mean_squared_error: 0.3273\nEpoch 5/10\n12037/12037 [==============================] - 76s 6ms/step - loss: 0.3442 - mean_squared_error: 0.3442\nEpoch 6/10\n12037/12037 [==============================] - 75s 6ms/step - loss: 0.3734 - mean_squared_error: 0.3734\nEpoch 7/10\n12037/12037 [==============================] - 74s 6ms/step - loss: 0.3285 - mean_squared_error: 0.3285\nEpoch 8/10\n12037/12037 [==============================] - 79s 7ms/step - loss: 0.3222 - mean_squared_error: 0.3222\nEpoch 9/10\n12037/12037 [==============================] - 73s 6ms/step - loss: 0.3271 - mean_squared_error: 0.3271\nEpoch 10/10\n12037/12037 [==============================] - 75s 6ms/step - loss: 0.3088 - mean_squared_error: 0.3088\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 55, "data": {"text/plain": "<keras.callbacks.History at 0x7fc6a4122320>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn2 = nnmodel2.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn2.reshape(5929,)))", "execution_count": 56, "outputs": [{"output_type": "stream", "text": "MSE: 0.24032697966728822\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel21 = Sequential()\nnnmodel21.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel21.add(Dropout(0.2))\nnnmodel21.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel21.add(Dropout(0.2))\nnnmodel21.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel21.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])\nnnmodel21.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)", "execution_count": 57, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 38s 3ms/step - loss: 0.6560 - mean_squared_error: 0.6560\nEpoch 2/10\n12037/12037 [==============================] - 38s 3ms/step - loss: 0.3230 - mean_squared_error: 0.3230\nEpoch 3/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.2955 - mean_squared_error: 0.2955\nEpoch 4/10\n12037/12037 [==============================] - 36s 3ms/step - loss: 0.2799 - mean_squared_error: 0.2799\nEpoch 5/10\n12037/12037 [==============================] - 36s 3ms/step - loss: 0.2485 - mean_squared_error: 0.2485\nEpoch 6/10\n12037/12037 [==============================] - 38s 3ms/step - loss: 0.2370 - mean_squared_error: 0.2370\nEpoch 7/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.2441 - mean_squared_error: 0.2441\nEpoch 8/10\n12037/12037 [==============================] - 36s 3ms/step - loss: 0.2727 - mean_squared_error: 0.2727\nEpoch 9/10\n12037/12037 [==============================] - 34s 3ms/step - loss: 0.2417 - mean_squared_error: 0.2417\nEpoch 10/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.2157 - mean_squared_error: 0.2157\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 57, "data": {"text/plain": "<keras.callbacks.History at 0x7fc61419ce48>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn21 = nnmodel21.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn21.reshape(5929,)))", "execution_count": 58, "outputs": [{"output_type": "stream", "text": "MSE: 0.21589163515140736\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel22 = Sequential()\nnnmodel22.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel22.add(Dropout(0.4))\nnnmodel22.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel22.add(Dropout(0.2))\nnnmodel22.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel22.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])\nnnmodel22.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)", "execution_count": 59, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 35s 3ms/step - loss: 0.6429 - mean_squared_error: 0.6429\nEpoch 2/10\n12037/12037 [==============================] - 33s 3ms/step - loss: 0.3767 - mean_squared_error: 0.3767\nEpoch 3/10\n12037/12037 [==============================] - 29s 2ms/step - loss: 0.3384 - mean_squared_error: 0.3384\nEpoch 4/10\n12037/12037 [==============================] - 27s 2ms/step - loss: 0.2843 - mean_squared_error: 0.2843\nEpoch 5/10\n12037/12037 [==============================] - 26s 2ms/step - loss: 0.2910 - mean_squared_error: 0.2910\nEpoch 6/10\n12037/12037 [==============================] - 9s 729us/step - loss: 0.2871 - mean_squared_error: 0.2871\nEpoch 7/10\n12037/12037 [==============================] - 19s 2ms/step - loss: 0.2907 - mean_squared_error: 0.2907\nEpoch 8/10\n12037/12037 [==============================] - 17s 1ms/step - loss: 0.2739 - mean_squared_error: 0.2739\nEpoch 9/10\n12037/12037 [==============================] - 7s 557us/step - loss: 0.2683 - mean_squared_error: 0.2683\nEpoch 10/10\n12037/12037 [==============================] - 8s 677us/step - loss: 0.2925 - mean_squared_error: 0.2925\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 59, "data": {"text/plain": "<keras.callbacks.History at 0x7fc5d175d908>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn22 = nnmodel22.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn22.reshape(5929,)))", "execution_count": 60, "outputs": [{"output_type": "stream", "text": "MSE: 0.2166728872335843\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Note on choosing batch size. <br>\nTypically, choosing a larger batch size would lead to less accurate predictions. But choosing a very small batch size would affect the training speed of the neural network. Here I stick with a batch size of 32. Through the two practices above, one may observe that increasing the batch size does not significantly improve the MSE. So I will stick with the batch size 32 in the model construction below. "}, {"metadata": {}, "cell_type": "code", "source": "nnmodel3 = Sequential()\nnnmodel3.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel3.add(Dropout(0.4))\nnnmodel3.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel3.add(Dropout(0.4))\nnnmodel3.add(Dense(10, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel3.add(Dropout(0.4))\nnnmodel3.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel3.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])", "execution_count": 61, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nnmodel3.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)", "execution_count": 62, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 98s 8ms/step - loss: 0.7740 - mean_squared_error: 0.7740\nEpoch 2/10\n12037/12037 [==============================] - 111s 9ms/step - loss: 0.5556 - mean_squared_error: 0.5556\nEpoch 3/10\n12037/12037 [==============================] - 111s 9ms/step - loss: 0.4723 - mean_squared_error: 0.4723\nEpoch 4/10\n12037/12037 [==============================] - 111s 9ms/step - loss: 0.4414 - mean_squared_error: 0.4414\nEpoch 5/10\n12037/12037 [==============================] - 113s 9ms/step - loss: 0.4502 - mean_squared_error: 0.4502\nEpoch 6/10\n12037/12037 [==============================] - 117s 10ms/step - loss: 0.4306 - mean_squared_error: 0.4306\nEpoch 7/10\n12037/12037 [==============================] - 116s 10ms/step - loss: 0.4310 - mean_squared_error: 0.4310\nEpoch 8/10\n12037/12037 [==============================] - 106s 9ms/step - loss: 0.4351 - mean_squared_error: 0.4351\nEpoch 9/10\n12037/12037 [==============================] - 115s 10ms/step - loss: 0.3969 - mean_squared_error: 0.3969\nEpoch 10/10\n12037/12037 [==============================] - 114s 9ms/step - loss: 0.3917 - mean_squared_error: 0.3917\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 62, "data": {"text/plain": "<keras.callbacks.History at 0x7fc61421c978>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn3 = nnmodel3.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn3.reshape(5929,)))", "execution_count": 63, "outputs": [{"output_type": "stream", "text": "MSE: 0.2321377457424366\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel31 = Sequential()\nnnmodel31.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel31.add(Dropout(0.2))\nnnmodel31.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel31.add(Dropout(0.2))\nnnmodel31.add(Dense(10, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel31.add(Dropout(0.2))\nnnmodel31.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel31.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])\nnnmodel31.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)", "execution_count": 64, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 47s 4ms/step - loss: 0.7784 - mean_squared_error: 0.7784\nEpoch 2/10\n12037/12037 [==============================] - 53s 4ms/step - loss: 0.4129 - mean_squared_error: 0.4129\nEpoch 3/10\n12037/12037 [==============================] - 50s 4ms/step - loss: 0.3881 - mean_squared_error: 0.3881\nEpoch 4/10\n12037/12037 [==============================] - 52s 4ms/step - loss: 0.3451 - mean_squared_error: 0.3451\nEpoch 5/10\n12037/12037 [==============================] - 50s 4ms/step - loss: 0.3511 - mean_squared_error: 0.3511\nEpoch 6/10\n12037/12037 [==============================] - 56s 5ms/step - loss: 0.3425 - mean_squared_error: 0.3425\nEpoch 7/10\n12037/12037 [==============================] - 55s 5ms/step - loss: 0.3230 - mean_squared_error: 0.3230\nEpoch 8/10\n12037/12037 [==============================] - 63s 5ms/step - loss: 0.3118 - mean_squared_error: 0.3118\nEpoch 9/10\n12037/12037 [==============================] - 62s 5ms/step - loss: 0.3154 - mean_squared_error: 0.3154\nEpoch 10/10\n12037/12037 [==============================] - 55s 5ms/step - loss: 0.2938 - mean_squared_error: 0.2938\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 64, "data": {"text/plain": "<keras.callbacks.History at 0x7fc5d0925278>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn31 = nnmodel31.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn31.reshape(5929,)))", "execution_count": 65, "outputs": [{"output_type": "stream", "text": "MSE: 0.2138056078898866\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel32 = Sequential()\nnnmodel32.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel32.add(Dropout(0.4))\nnnmodel32.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel32.add(Dropout(0.3))\nnnmodel32.add(Dense(10, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel32.add(Dropout(0.2))\nnnmodel32.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel32.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])\nnnmodel32.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)", "execution_count": 66, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 60s 5ms/step - loss: 0.7409 - mean_squared_error: 0.7409\nEpoch 2/10\n12037/12037 [==============================] - 56s 5ms/step - loss: 0.4864 - mean_squared_error: 0.4864\nEpoch 3/10\n12037/12037 [==============================] - 58s 5ms/step - loss: 0.4000 - mean_squared_error: 0.4000\nEpoch 4/10\n12037/12037 [==============================] - 58s 5ms/step - loss: 0.4033 - mean_squared_error: 0.4033\nEpoch 5/10\n12037/12037 [==============================] - 54s 5ms/step - loss: 0.3925 - mean_squared_error: 0.3925\nEpoch 6/10\n12037/12037 [==============================] - 48s 4ms/step - loss: 0.3388 - mean_squared_error: 0.3388\nEpoch 7/10\n12037/12037 [==============================] - 37s 3ms/step - loss: 0.3514 - mean_squared_error: 0.3514\nEpoch 8/10\n12037/12037 [==============================] - 57s 5ms/step - loss: 0.3466 - mean_squared_error: 0.3466\nEpoch 9/10\n12037/12037 [==============================] - 54s 5ms/step - loss: 0.3544 - mean_squared_error: 0.3544\nEpoch 10/10\n12037/12037 [==============================] - 54s 4ms/step - loss: 0.3593 - mean_squared_error: 0.3593\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 66, "data": {"text/plain": "<keras.callbacks.History at 0x7fc5d0148ef0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn32 = nnmodel32.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn32.reshape(5929,)))", "execution_count": 67, "outputs": [{"output_type": "stream", "text": "MSE: 0.21553163663522576\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel4 = Sequential()\nnnmodel4.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel4.add(Dropout(0.4))\nnnmodel4.add(Dense(30, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel4.add(Dropout(0.4))\nnnmodel4.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel4.add(Dropout(0.4))\nnnmodel4.add(Dense(10, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel4.add(Dropout(0.4))\nnnmodel4.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel4.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])", "execution_count": 68, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nnmodel4.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)", "execution_count": 69, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 153s 13ms/step - loss: 0.7696 - mean_squared_error: 0.7696\nEpoch 2/10\n12037/12037 [==============================] - 142s 12ms/step - loss: 0.4843 - mean_squared_error: 0.4843\nEpoch 3/10\n12037/12037 [==============================] - 153s 13ms/step - loss: 0.4417 - mean_squared_error: 0.4417\nEpoch 4/10\n12037/12037 [==============================] - 149s 12ms/step - loss: 0.4367 - mean_squared_error: 0.4367\nEpoch 5/10\n12037/12037 [==============================] - 93s 8ms/step - loss: 0.4713 - mean_squared_error: 0.4713\nEpoch 6/10\n12037/12037 [==============================] - 146s 12ms/step - loss: 0.4219 - mean_squared_error: 0.4219\nEpoch 7/10\n12037/12037 [==============================] - 149s 12ms/step - loss: 0.3898 - mean_squared_error: 0.3898\nEpoch 8/10\n12037/12037 [==============================] - 148s 12ms/step - loss: 0.3593 - mean_squared_error: 0.3593\nEpoch 9/10\n12037/12037 [==============================] - 152s 13ms/step - loss: 0.3354 - mean_squared_error: 0.3354\nEpoch 10/10\n12037/12037 [==============================] - 151s 13ms/step - loss: 0.3874 - mean_squared_error: 0.3874\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 69, "data": {"text/plain": "<keras.callbacks.History at 0x7fc5d0c4ab70>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn4 = nnmodel4.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn4.reshape(5929,)))", "execution_count": 70, "outputs": [{"output_type": "stream", "text": "MSE: 0.24058116647477373\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "nnmodel41 = Sequential()\nnnmodel41.add(Dense(30, input_dim = 31, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel41.add(Dropout(0.2))\nnnmodel41.add(Dense(30, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel41.add(Dropout(0.2))\nnnmodel41.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel41.add(Dropout(0.2))\nnnmodel41.add(Dense(10, kernel_initializer = 'normal', activation = 'relu'))\nnnmodel41.add(Dropout(0.2))\nnnmodel41.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\nnnmodel41.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])\nnnmodel41.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)", "execution_count": 71, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n12037/12037 [==============================] - 76s 6ms/step - loss: 0.6314 - mean_squared_error: 0.6314\nEpoch 2/10\n12037/12037 [==============================] - 76s 6ms/step - loss: 0.3993 - mean_squared_error: 0.3993\nEpoch 3/10\n12037/12037 [==============================] - 75s 6ms/step - loss: 0.3301 - mean_squared_error: 0.3301\nEpoch 4/10\n12037/12037 [==============================] - 77s 6ms/step - loss: 0.3416 - mean_squared_error: 0.3416\nEpoch 5/10\n12037/12037 [==============================] - 73s 6ms/step - loss: 0.3123 - mean_squared_error: 0.3123\nEpoch 6/10\n12037/12037 [==============================] - 76s 6ms/step - loss: 0.3311 - mean_squared_error: 0.3311\nEpoch 7/10\n12037/12037 [==============================] - 75s 6ms/step - loss: 0.3118 - mean_squared_error: 0.3118\nEpoch 8/10\n12037/12037 [==============================] - 70s 6ms/step - loss: 0.3065 - mean_squared_error: 0.3065\nEpoch 9/10\n12037/12037 [==============================] - 76s 6ms/step - loss: 0.3025 - mean_squared_error: 0.3025\nEpoch 10/10\n12037/12037 [==============================] - 77s 6ms/step - loss: 0.3075 - mean_squared_error: 0.3075\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 71, "data": {"text/plain": "<keras.callbacks.History at 0x7fc5b3275c18>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_test_nn41 = nnmodel41.predict(x_test_std)\nprint('MSE:', mean_squared_error(y_test_std, y_test_nn41.reshape(5929,)))", "execution_count": 72, "outputs": [{"output_type": "stream", "text": "MSE: 0.23246362675952806\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "As observed above, having more layer is not equivalent to better prediction. The model with the best MSE score is neural network model 31. The MSE of it is approximately 0.214. It has batch size 64 and it has the following layers: \n\n| Layer | # of Inputs | # of Outputs | Activation Function | Dropout Rate |\n|------|------|------|------|------|\n| 1 | 31 | 30 | Relu | 0.2 |\n| 2 | 30 | 20 | Relu | 0.2 |\n| 3 | 20 | 10 | Relu | 0.2 |\n| 4 | 10 | 1 | Linear| N/A |\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3 Model Evaluation\nModel evaluation is based on the score, specifically mean squared error (MSE) on the test set. Based on the model trained, how does the model perform on an unseen dataset. The mean squared error is calculated by the mean of the square of errors. It is an effective way to measure how different is the prediction from the actual value. The equation of MSE is given by: <br>\n$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2$ <br>\nThe following table compares the performance of different model algorithms: \n\n| Model | MSE Score | Hyperparameter/Layers | Interpretable | \n|------|------|------|------| \n| Linear regression with LASSO regularization | $\\approx 0.240$ | $\\alpha = 10^{-10}$ | Coefficients can be used to interpret feature importance |\n| Random Forest | $\\approx 0.206$ | # of trees in forest = 500, maximum depth of tree = 20, maximum feature = 20 | Low |\n| Neural Network | $\\approx 0.213$ | 4 layers in total (Neurons 31>30>20>10>1) | Low |\n\nThe model that performs the best prediction on the unseen dataset is the random forest model. Using this model, one can predict a player's wage given the player's profile with the least MSE. Sometimes there might be missing data in the player's profile. The idea of how to deal with missing data is described in the data preprocessing section. Then one can easily predict the player's wage with little error. For the club owners and managers, they can decide on the wages of the player based on this player's attributes. This is a reliable way to negotiate with player without underpaying or overpaying. <br>\nHowever, neural network model does not provide a understanding on how to interpret FIFA players' wages. It would be good to know intuitively which feature(s) of the players affect their wages more than others. This analysis of feature importance will be discussed in the next section.  "}, {"metadata": {}, "cell_type": "markdown", "source": "### 4 Model Deployment\nIn the previous section, I have discussed which model to use in predicting the players' wages. Now let's turn to an intuitive side\n\n| Feature | Description | Coefficient | \n|------|------|------|\n| Age | Age | $\\approx 0.121$ | \n| Value | Current Market Value | $\\approx 0.877$ | \n| Volleys | Rating on Volleys | $\\approx 0.00932$ |\n\nBased on the coefficient of LASSO, the table above lists the top three features affecting the wage of the FIFA player : age of the player, current market value of the player and the rating of volleys. Although LASSO is the worst at predicting player wage, it provides some insights on interpreting the coefficients. When evaluating the wage of player, one may use these three features together, rather than using one feature, to estimate the wage of a player. For club seeking recruitment, they can use these three features to estimate the wages of players and check whether they would be a good match. \n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 5 Insights\nFor club owners and managers, if they're looking for a player to recruit, they may search for a player that matches them the best. While players with better performance and ability are always preferred, clubs cannot afford to recruit a player with a wage beyond their budget. So this project is useful for the soccer club owners in two ways: <br>\n\n----\n1. During the early recruitment stage, use the analysis on the feature importance in the previous section to briefly estimate the wage of the all players and seek for the most appropriate candidates in terms of both affordability and performance\n2. Once appropriate candidate is found, the club may use the random forest model to work out the best wage prediction to gain an advantage during the negotiations. \n\n-----\n\nWith that being said, predicting the wages of FIFA players is very beneficial for the club owners and managers to maximize their benefit. <br>\n<br>\nThere's also an insight for future research. Soccer players have different positions, and different positions may value different attributes. Further study could divide the players' data into categories of positions to achieve better understanding of players' wage. \n"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}