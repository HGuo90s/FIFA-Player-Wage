{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Construction and Training\n",
    "This notebook denotes the process of which the models are constructed and trained on the data. The following is the table of contents of this notebook. <br>\n",
    "\n",
    "--------\n",
    "1. Data Preprocessing\n",
    "2. Model Setup and Training <br>\n",
    "  2.1 Linear Regression and LASSO <br>\n",
    "  2.2 Random Forest <br>\n",
    "  2.3 Neural Network\n",
    "3. Model Evaluation\n",
    "4. Model Deployment\n",
    "5. Insights\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Data Preprocessing\n",
    "Load the data before constructing the model. The goal of this section is to convert data to what a perfered type and deal with missing values. \n",
    "\n",
    "-------\n",
    "Here is what I have done in the data preprocessing stage: \n",
    "1. Convert the data into usable data <br>\n",
    "  1.1 Some data involves unnecessary characters such as \"€\". These symbols are removed or replace. <br> \n",
    "  1.2 Convert data into integers. For example, height data could be 5'11''. Then the data is converted to inches ($5\\times12+11$) <br>\n",
    "2. Replace missing values <br>\n",
    "  1.1 For some attributes that could not be zero (e.g. Height), the missing values are replaced with the median of the existing set <br>\n",
    "  1.2 For some attributes that could be zero (e.g. gate keeper handling), the missing values are replaced by zero. <br>\n",
    "  \n",
    "-------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200121022330-0000\n",
      "KERNEL_ID = 37f8150e-a0e8-4708-a51c-57951b0feaab\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Potential</th>\n",
       "      <th>Club</th>\n",
       "      <th>...</th>\n",
       "      <th>Composure</th>\n",
       "      <th>Marking</th>\n",
       "      <th>StandingTackle</th>\n",
       "      <th>SlidingTackle</th>\n",
       "      <th>GKDiving</th>\n",
       "      <th>GKHandling</th>\n",
       "      <th>GKKicking</th>\n",
       "      <th>GKPositioning</th>\n",
       "      <th>GKReflexes</th>\n",
       "      <th>Release Clause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>158023</td>\n",
       "      <td>L. Messi</td>\n",
       "      <td>31</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/158023.png</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>https://cdn.sofifa.org/flags/52.png</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>FC Barcelona</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>€226.5M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20801</td>\n",
       "      <td>Cristiano Ronaldo</td>\n",
       "      <td>33</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/20801.png</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>https://cdn.sofifa.org/flags/38.png</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>Juventus</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>€127.1M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>190871</td>\n",
       "      <td>Neymar Jr</td>\n",
       "      <td>26</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/190871.png</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>https://cdn.sofifa.org/flags/54.png</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>Paris Saint-Germain</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>€228.1M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>193080</td>\n",
       "      <td>De Gea</td>\n",
       "      <td>27</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/193080.png</td>\n",
       "      <td>Spain</td>\n",
       "      <td>https://cdn.sofifa.org/flags/45.png</td>\n",
       "      <td>91</td>\n",
       "      <td>93</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>€138.6M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>192985</td>\n",
       "      <td>K. De Bruyne</td>\n",
       "      <td>27</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/192985.png</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>https://cdn.sofifa.org/flags/7.png</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>Manchester City</td>\n",
       "      <td>...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>€196.4M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      ID               Name  Age  \\\n",
       "0           0  158023           L. Messi   31   \n",
       "1           1   20801  Cristiano Ronaldo   33   \n",
       "2           2  190871          Neymar Jr   26   \n",
       "3           3  193080             De Gea   27   \n",
       "4           4  192985       K. De Bruyne   27   \n",
       "\n",
       "                                            Photo Nationality  \\\n",
       "0  https://cdn.sofifa.org/players/4/19/158023.png   Argentina   \n",
       "1   https://cdn.sofifa.org/players/4/19/20801.png    Portugal   \n",
       "2  https://cdn.sofifa.org/players/4/19/190871.png      Brazil   \n",
       "3  https://cdn.sofifa.org/players/4/19/193080.png       Spain   \n",
       "4  https://cdn.sofifa.org/players/4/19/192985.png     Belgium   \n",
       "\n",
       "                                  Flag  Overall  Potential  \\\n",
       "0  https://cdn.sofifa.org/flags/52.png       94         94   \n",
       "1  https://cdn.sofifa.org/flags/38.png       94         94   \n",
       "2  https://cdn.sofifa.org/flags/54.png       92         93   \n",
       "3  https://cdn.sofifa.org/flags/45.png       91         93   \n",
       "4   https://cdn.sofifa.org/flags/7.png       91         92   \n",
       "\n",
       "                  Club  ... Composure Marking StandingTackle  SlidingTackle  \\\n",
       "0         FC Barcelona  ...      96.0    33.0           28.0           26.0   \n",
       "1             Juventus  ...      95.0    28.0           31.0           23.0   \n",
       "2  Paris Saint-Germain  ...      94.0    27.0           24.0           33.0   \n",
       "3    Manchester United  ...      68.0    15.0           21.0           13.0   \n",
       "4      Manchester City  ...      88.0    68.0           58.0           51.0   \n",
       "\n",
       "  GKDiving  GKHandling  GKKicking  GKPositioning GKReflexes Release Clause  \n",
       "0      6.0        11.0       15.0           14.0        8.0        €226.5M  \n",
       "1      7.0        11.0       15.0           14.0       11.0        €127.1M  \n",
       "2      9.0         9.0       15.0           15.0       11.0        €228.1M  \n",
       "3     90.0        85.0       87.0           88.0       94.0        €138.6M  \n",
       "4     15.0        13.0        5.0           10.0       13.0        €196.4M  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_65d4a81f39cd4ebeba4f9b0b9d168ea8 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='TH6pdPiMrAco2paAMq7JAw5BIZIKjf59H5KdNRoO5F74',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_65d4a81f39cd4ebeba4f9b0b9d168ea8.get_object(Bucket='couseraibmdatascienceproject-donotdelete-pr-fff0ca2ef4bcja',Key='data.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "# If you are reading an Excel file into a pandas DataFrame, replace `read_csv` by `read_excel` in the next statement.\n",
    "df = pd.read_csv(body)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Wage from object to integer\n",
    "df['Wage'] = df['Wage'].astype(str)\n",
    "df['Wage'] = df['Wage'].str.replace('€', '')\n",
    "df['Wage'] = df['Wage'].str.replace('K', '')\n",
    "df['Wage'] = df['Wage'].astype(int)\n",
    "df['Wage'].head()\n",
    "df_1 = df[df.Wage != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Value from object to int type\n",
    "valuecol = df_1.columns.get_loc('Value')\n",
    "df_1.loc[:, 'Value'] = df_1.loc[:, 'Value'].astype(str)\n",
    "df_1['Value'] = df_1['Value'].str.replace('€', '')\n",
    "\n",
    "Ks = np.where(df_1['Value'].str.contains('K'))[0]\n",
    "Ks = np.ndarray.tolist(Ks)\n",
    "df_1.iloc[Ks,valuecol] = df_1.iloc[Ks,valuecol].str.replace('K', '')\n",
    "\n",
    "                                                                                                                                   \n",
    "Ms = np.where(df_1['Value'].str.contains('M'))[0]\n",
    "Ms = np.ndarray.tolist(Ms)\n",
    "df_1.iloc[Ms,valuecol] = df_1.iloc[Ms,valuecol].str.replace('M', '')\n",
    "df_1.iloc[Ms,valuecol] = df_1.iloc[Ms,valuecol].astype(float) * 1000\n",
    "\n",
    "df_1['Value'] = df_1['Value'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Height to int type\n",
    "df_1copy = df_1.copy()\n",
    "df_1copy['Height'] = df_1copy['Height'].astype(str)\n",
    "NAs = np.where(df_1copy['Height'] == 'nan')\n",
    "NAs = NAs[0]\n",
    "inches = df_1copy['Height'].str[0]\n",
    "feet = df_1copy['Height'].str[2:]\n",
    "inches[inches == 'n'] = '0'\n",
    "feet[feet == 'n'] = '0'\n",
    "df_1copy['Height'] = inches.astype(int) * 12 + feet.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Height'] = df_1copy['Height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hei = df_1.columns.get_loc('Height')\n",
    "df_1.iloc[np.ndarray.tolist(NAs), hei] = df_1.Height[df_1.Height != 0].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert weight to int type\n",
    "df_1['Weight'] = df_1['Weight'].astype(str)\n",
    "df_1['Weight'] = df_1['Weight'].str.replace('lbs','')\n",
    "df_1.Weight[df_1.Weight == 'nan'] = '0'\n",
    "df_1['Weight'] = df_1['Weight'].astype(int)\n",
    "df_1.Weight[df_1.Weight == 0] = df_1.Weight[df_1.Weight != 0].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = ['Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', \n",
    "         'LongPassing', 'BallControl', 'Acceleration','SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', \n",
    "         'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', \n",
    "         'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', \n",
    "         'GKPositioning', 'GKReflexes']\n",
    "df_1[records] = df_1[records].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wage</th>\n",
       "      <th>Age</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Potential</th>\n",
       "      <th>Value</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Crossing</th>\n",
       "      <th>Finishing</th>\n",
       "      <th>HeadingAccuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>Penalties</th>\n",
       "      <th>Composure</th>\n",
       "      <th>Marking</th>\n",
       "      <th>StandingTackle</th>\n",
       "      <th>SlidingTackle</th>\n",
       "      <th>GKDiving</th>\n",
       "      <th>GKHandling</th>\n",
       "      <th>GKKicking</th>\n",
       "      <th>GKPositioning</th>\n",
       "      <th>GKReflexes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>565</td>\n",
       "      <td>31</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>110500</td>\n",
       "      <td>67.0</td>\n",
       "      <td>159</td>\n",
       "      <td>84.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>405</td>\n",
       "      <td>33</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>77000</td>\n",
       "      <td>74.0</td>\n",
       "      <td>183</td>\n",
       "      <td>84.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>290</td>\n",
       "      <td>26</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>118500</td>\n",
       "      <td>69.0</td>\n",
       "      <td>150</td>\n",
       "      <td>79.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>260</td>\n",
       "      <td>27</td>\n",
       "      <td>91</td>\n",
       "      <td>93</td>\n",
       "      <td>72000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>168</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>355</td>\n",
       "      <td>27</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>102000</td>\n",
       "      <td>71.0</td>\n",
       "      <td>154</td>\n",
       "      <td>93.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wage  Age  Overall  Potential   Value  Height  Weight  Crossing  Finishing  \\\n",
       "0   565   31       94         94  110500    67.0     159      84.0       95.0   \n",
       "1   405   33       94         94   77000    74.0     183      84.0       94.0   \n",
       "2   290   26       92         93  118500    69.0     150      79.0       87.0   \n",
       "3   260   27       91         93   72000    76.0     168      17.0       13.0   \n",
       "4   355   27       91         92  102000    71.0     154      93.0       82.0   \n",
       "\n",
       "   HeadingAccuracy  ...  Penalties  Composure  Marking  StandingTackle  \\\n",
       "0             70.0  ...       75.0       96.0     33.0            28.0   \n",
       "1             89.0  ...       85.0       95.0     28.0            31.0   \n",
       "2             62.0  ...       81.0       94.0     27.0            24.0   \n",
       "3             21.0  ...       40.0       68.0     15.0            21.0   \n",
       "4             55.0  ...       79.0       88.0     68.0            58.0   \n",
       "\n",
       "   SlidingTackle  GKDiving  GKHandling  GKKicking  GKPositioning  GKReflexes  \n",
       "0           26.0       6.0        11.0       15.0           14.0         8.0  \n",
       "1           23.0       7.0        11.0       15.0           14.0        11.0  \n",
       "2           33.0       9.0         9.0       15.0           15.0        11.0  \n",
       "3           13.0      90.0        85.0       87.0           88.0        94.0  \n",
       "4           51.0      15.0        13.0        5.0           10.0        13.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allcol = ['Wage', 'Age', 'Overall', 'Potential', 'Value', 'Height', 'Weight', 'Crossing', 'Finishing', \n",
    "              'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', \n",
    "              'BallControl', 'Acceleration','SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', \n",
    "              'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', \n",
    "              'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', \n",
    "              'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes' ]\n",
    "df_1[allcol].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['Age', 'Overall', 'Potential', 'Value', 'Height', 'Weight', 'Crossing', 'Finishing', \n",
    "              'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', \n",
    "              'BallControl', 'Acceleration','SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', \n",
    "              'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', \n",
    "              'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', \n",
    "              'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_test, y_tr, y_test = train_test_split(df_1[attributes], df_1['Wage'], test_size = 0.33)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_tr, y_tr, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two ways to scale the data\n",
    "scaler = StandardScaler()\n",
    "x_train_std = scaler.fit_transform(x_train)\n",
    "x_cv_std = scaler.transform(x_cv)\n",
    "x_test_std = scaler.transform(x_test)\n",
    "x_tr_std = scaler.fit_transform(x_tr)\n",
    "y_train_std = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_cv_std = scaler.transform(np.array(y_cv).reshape(-1, 1))\n",
    "y_tr_std = scaler.fit_transform(np.array(y_tr).reshape(-1, 1))\n",
    "y_test_std = scaler.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "normalize = MinMaxScaler()\n",
    "x_train_nor = normalize.fit_transform(x_train)\n",
    "x_cv_nor = normalize.transform(x_cv)\n",
    "x_test_nor = normalize.transform(x_test)\n",
    "x_tr_nor = normalize.fit_transform(x_tr)\n",
    "y_tr_nor = normalize.fit_transform(np.array(y_tr).reshape(-1, 1))\n",
    "y_test_nor = normalize.transform(np.array(y_test).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're two ways to re-scale the data set. Standardization and normalization. Standardization rescales the data so that it has a mean of 0 and standard deviation 1; normalization rescale the attribute to range between 0 and 1. It's difficult to tell which method is better. For most of the time, this project will stick with standardization. <br>\n",
    "$\\textbf{Notice}$ that the dataset above is divided to three categories: training set, cross validation set and testing set. The general idea behind these three categories is: training set is used for first training the model, cross-validation set is used for hyperparameter tuning (especially for random forest), testing set is used for comparing the performance across algorithms. The model is first trained on the training set. Then the scores on the cross validation set is used to determine which hyperparameter is the best to use. Finally, the model is re-trained on both the training and cross validation set (x_tr, y_tr), and the performances across different algorithms (regression vs. random forest vs. neural network) will be evaluated based on the scores of the test test. <br>\n",
    "$\\textbf{With}$ that being said, sometimes the \"tr\" set (training + cross validation set) will be used directly. This is because some commands in Python enables user to directly setup the cross validation set within simple lines of command, and the user may access the score on the cv set directly without having to train the model only on the training set. The spliting of the dataset is random, so different spliting would presumably not have an effect on the evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Model Construction\n",
    "The following models are considered: <br>\n",
    "Linear Regression & LASSO<br>\n",
    "Random Forest <br>\n",
    "Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear Regression & LASSO\n",
    "The project begin by implementing a basic linear regression model on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lmmodel_1 = lm.fit(x_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[ 1.03317410e-01 -3.64529083e-02  4.62619948e-02  8.44569999e-01\n",
      "   3.55947543e-02  4.81157485e-03  5.54196715e-02 -4.38959588e-02\n",
      "   2.71925495e-02 -8.87269575e-03 -2.63101161e-03  2.70219501e-02\n",
      "   1.87075269e-02 -5.85901493e-02 -1.41724377e-02  1.82654119e-03\n",
      "  -2.02402430e-02 -2.66560470e-04 -8.45246634e-03 -5.20825338e-03\n",
      "   3.74720243e-02  3.98214715e-02 -8.98982459e-04 -5.18530061e-02\n",
      "  -2.03965981e-02  2.77400297e-02  6.91912607e-04  7.49904484e-03\n",
      "   1.01144133e-02 -1.55447518e-02  2.27302036e-02 -9.80688481e-03\n",
      "  -3.31925918e-03 -3.87287503e-02  8.66805369e-02  1.86632933e-02\n",
      "   1.57100438e-02  1.48874832e-02 -2.65165605e-02  5.79314519e-03]]\n",
      "Intercept: [2.95154411e-17]\n"
     ]
    }
   ],
   "source": [
    "coefficients = lmmodel_1.coef_\n",
    "intercept = lmmodel_1.intercept_\n",
    "print('Coefficients:', coefficients)\n",
    "print('Intercept:', intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv_pred1 = lmmodel_1.predict(x_cv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training: 0.2543415532295527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE on training:', mean_squared_error(y_cv_std, y_cv_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO is a regression analysis that performs variable selection and regularization. Compared with the least square linear models, LASSO penalizes non-zero coefficients. In the commands below, the model is directly trained on the \"tr\" set because the command GridSearchCV could specify the means of cross-validation (here k-fold cross validation is used). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: {'alpha': 0.001}\n",
      "MSE: -0.24770263230526066\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "param = {'alpha': [1e-20, 1e-10, 1e-3, 1e-1, 1, 5, 20]}\n",
    "lasso_regressor = GridSearchCV(lasso, param, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "lasso_regressor.fit(x_tr_std, y_tr_std)\n",
    "print('Best hyperparameter:', lasso_regressor.best_params_)\n",
    "print('MSE:', lasso_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO outperforms linear regression model in terms of MSE. So let's test the MSE of prediction of LASSO using the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "param = {'alpha': [1e-20, 1e-10, 1e-3, 1e-1, 1, 5, 20]}\n",
    "lasso_regressor = GridSearchCV(lasso, param, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "lasso_regressor.fit(x_tr_std, y_tr_std)\n",
    "print(lasso_regressor.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.2564797691067471\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_lasso = lasso_regressor.predict(x_test_std)\n",
    "print('MSE', mean_squared_error(y_test_std, y_test_pred_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2564797691067471\n",
      "Coefficients [ 8.83515045e-02 -1.43532371e-02  3.20608171e-02  8.44866446e-01\n",
      "  2.54287424e-02  7.60330741e-03  3.38180650e-02 -1.67879627e-02\n",
      "  7.15094347e-03 -0.00000000e+00  2.22881861e-03  1.61928301e-02\n",
      "  1.74663508e-02 -5.53574204e-02 -2.07566492e-02  0.00000000e+00\n",
      " -1.61406761e-02  0.00000000e+00 -0.00000000e+00 -5.54758165e-03\n",
      "  2.25407789e-02  1.87835492e-02  4.05657321e-03 -5.47375219e-02\n",
      " -1.84990426e-02  1.39479241e-02  1.02652107e-03  0.00000000e+00\n",
      "  2.83955293e-03 -2.84788679e-04  3.68042478e-02 -4.83997210e-03\n",
      "  0.00000000e+00 -0.00000000e+00  6.58383952e-02  0.00000000e+00\n",
      "  1.41235565e-02  0.00000000e+00 -0.00000000e+00  0.00000000e+00] Intercept: [-3.99698276e-17]\n"
     ]
    }
   ],
   "source": [
    "lasso_coeff = Lasso(alpha = 0.001)\n",
    "lasso_coeff = lasso_coeff.fit(x_tr_std, y_tr_std)\n",
    "y_test_pred_lasso_coeff = lasso_coeff.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_pred_lasso_coeff))\n",
    "print('Coefficients', lasso_coeff.coef_, 'Intercept:', lasso_coeff.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest\n",
    "Random Forest is one of the machine learning methods. It is based on the idea of decision trees, but it's more effective than decision tree as a result of bagging. In case of random forest, we will not use GridSearchCV, which takes too long to complete (it's exhaustive searching combinations). Hence I will focus on running models on the training set first and evaluate the best hyperparameter using scores on the cross validation set. Then I will use the best hyperparameter to train the \"tr\" set and obtain the score on the test set. The best score is the smallest MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.24975180788188464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_1 = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "rf_1 = rf_1.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred1 = rf_1.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2889370790000581\n"
     ]
    }
   ],
   "source": [
    "rf_2 = RandomForestRegressor(n_estimators = 100, max_depth = 10, max_features = 'log2', min_samples_split = 3, min_samples_leaf = 2, random_state = 0)\n",
    "rf_2 = rf_2.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred2 = rf_2.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.28213573438778594\n"
     ]
    }
   ],
   "source": [
    "rf_3 = RandomForestRegressor(n_estimators = 1000, max_depth = 10, max_features = 'log2', random_state = 0)\n",
    "rf_3 = rf_3.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred3 = rf_3.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2929437116488881\n"
     ]
    }
   ],
   "source": [
    "rf_4 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 'log2', random_state = 0)\n",
    "rf_4 = rf_4.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred4 = rf_4.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2792108830030044\n"
     ]
    }
   ],
   "source": [
    "rf_5 = RandomForestRegressor(n_estimators = 100, max_depth = 30, max_features = 'log2', random_state = 0)\n",
    "rf_5 = rf_5.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred5 = rf_5.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2829231260455778\n"
     ]
    }
   ],
   "source": [
    "rf_6 = RandomForestRegressor(n_estimators = 100, max_depth = 40, max_features = 'log2', random_state = 0)\n",
    "rf_6 = rf_6.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred6 = rf_6.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.24963987289361042\n"
     ]
    }
   ],
   "source": [
    "rf_7 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 20, random_state = 0)\n",
    "rf_7 = rf_7.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred7 = rf_7.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2476407661132925\n"
     ]
    }
   ],
   "source": [
    "rf_8 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 30, random_state = 0)\n",
    "rf_8 = rf_8.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred8 = rf_8.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.24985650824394431\n"
     ]
    }
   ],
   "source": [
    "rf_9 = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 40, random_state = 0)\n",
    "rf_9 = rf_9.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred9 = rf_9.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.24929560575149673\n"
     ]
    }
   ],
   "source": [
    "rf_10 = RandomForestRegressor(n_estimators = 500, max_depth = 20, max_features = 40, random_state = 0)\n",
    "rf_10 = rf_10.fit(x_train_std, y_train_std)\n",
    "y_cv_rfpred10 = rf_10.predict(x_cv_std)\n",
    "print('MSE:', mean_squared_error(y_cv_std, y_cv_rfpred10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on MSE, we use the following hyperparameters. \n",
    "\n",
    "-----\n",
    "n_estimators= 100 <br>\n",
    "max_depth = 20 <br>\n",
    "max_features = 30\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2033254089306992\n"
     ]
    }
   ],
   "source": [
    "rf_fin = RandomForestRegressor(n_estimators = 100, max_depth = 20, max_features = 30, random_state = 0)\n",
    "rf_fin = rf_fin.fit(x_tr_std, y_tr_std)\n",
    "y_test_rfpred = rf_fin.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_rfpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE score on the test set is very good. This score is better than the score of LASSO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Neural Network\n",
    "Neural network is an effective deep learning method. Compared with the two previous models (regression, random forest), neural network is more difficult to interpret. The following codes examines this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel = Sequential()\n",
    "nnmodel.add(Dense(50, input_dim = 40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel.add(Dense(1, kernel_initializer = 'normal'))\n",
    "nnmodel.compile(loss = 'mean_squared_error', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12037/12037 [==============================] - 40s 3ms/step - loss: 0.4648\n",
      "Epoch 2/10\n",
      "12037/12037 [==============================] - 39s 3ms/step - loss: 0.2710\n",
      "Epoch 3/10\n",
      "12037/12037 [==============================] - 39s 3ms/step - loss: 0.2503\n",
      "Epoch 4/10\n",
      "12037/12037 [==============================] - 40s 3ms/step - loss: 0.2421\n",
      "Epoch 5/10\n",
      "12037/12037 [==============================] - 40s 3ms/step - loss: 0.2344\n",
      "Epoch 6/10\n",
      "12037/12037 [==============================] - 37s 3ms/step - loss: 0.2298\n",
      "Epoch 7/10\n",
      "12037/12037 [==============================] - 39s 3ms/step - loss: 0.2217\n",
      "Epoch 8/10\n",
      "12037/12037 [==============================] - 38s 3ms/step - loss: 0.2229\n",
      "Epoch 9/10\n",
      "12037/12037 [==============================] - 40s 3ms/step - loss: 0.2253\n",
      "Epoch 10/10\n",
      "12037/12037 [==============================] - 39s 3ms/step - loss: 0.2216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5e28aa780>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.25397525358233153\n"
     ]
    }
   ],
   "source": [
    "y_test_nn1 = nnmodel.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_nn1.reshape(5929,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12037/12037 [==============================] - 21s 2ms/step - loss: 0.5339\n",
      "Epoch 2/10\n",
      "12037/12037 [==============================] - 19s 2ms/step - loss: 0.2979\n",
      "Epoch 3/10\n",
      "12037/12037 [==============================] - 20s 2ms/step - loss: 0.2597\n",
      "Epoch 4/10\n",
      "12037/12037 [==============================] - 20s 2ms/step - loss: 0.2514\n",
      "Epoch 5/10\n",
      "12037/12037 [==============================] - 19s 2ms/step - loss: 0.2492\n",
      "Epoch 6/10\n",
      "12037/12037 [==============================] - 20s 2ms/step - loss: 0.2421\n",
      "Epoch 7/10\n",
      "12037/12037 [==============================] - 20s 2ms/step - loss: 0.2382\n",
      "Epoch 8/10\n",
      "12037/12037 [==============================] - 19s 2ms/step - loss: 0.2338\n",
      "Epoch 9/10\n",
      "12037/12037 [==============================] - 20s 2ms/step - loss: 0.2326\n",
      "Epoch 10/10\n",
      "12037/12037 [==============================] - 20s 2ms/step - loss: 0.2297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f25f987b8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel11 = Sequential()\n",
    "nnmodel11.add(Dense(50, input_dim = 40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel11.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "nnmodel11.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "nnmodel11.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2366867855021129\n"
     ]
    }
   ],
   "source": [
    "y_test_nn11 = nnmodel11.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_nn11.reshape(5929,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel2 = Sequential()\n",
    "nnmodel2.add(Dense(50, input_dim = 40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel2.add(Dropout(0.4))\n",
    "nnmodel2.add(Dense(40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel2.add(Dropout(0.4))\n",
    "nnmodel2.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "nnmodel2.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12037/12037 [==============================] - 87s 7ms/step - loss: 0.5417 - mean_squared_error: 0.5417\n",
      "Epoch 2/10\n",
      "12037/12037 [==============================] - 88s 7ms/step - loss: 0.3339 - mean_squared_error: 0.3339\n",
      "Epoch 3/10\n",
      "12037/12037 [==============================] - 86s 7ms/step - loss: 0.3406 - mean_squared_error: 0.3406\n",
      "Epoch 4/10\n",
      "12037/12037 [==============================] - 89s 7ms/step - loss: 0.3276 - mean_squared_error: 0.3276\n",
      "Epoch 5/10\n",
      "12037/12037 [==============================] - 90s 8ms/step - loss: 0.3123 - mean_squared_error: 0.3123\n",
      "Epoch 6/10\n",
      "12037/12037 [==============================] - 81s 7ms/step - loss: 0.3007 - mean_squared_error: 0.3007\n",
      "Epoch 7/10\n",
      "12037/12037 [==============================] - 86s 7ms/step - loss: 0.3028 - mean_squared_error: 0.3028\n",
      "Epoch 8/10\n",
      "12037/12037 [==============================] - 87s 7ms/step - loss: 0.3004 - mean_squared_error: 0.3004\n",
      "Epoch 9/10\n",
      "12037/12037 [==============================] - 87s 7ms/step - loss: 0.2914 - mean_squared_error: 0.2914\n",
      "Epoch 10/10\n",
      "12037/12037 [==============================] - 87s 7ms/step - loss: 0.3025 - mean_squared_error: 0.3025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f804c2306d8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel2.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.23937945630262367\n"
     ]
    }
   ],
   "source": [
    "y_test_nn2 = nnmodel2.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_nn2.reshape(5929,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12037/12037 [==============================] - 47s 4ms/step - loss: 0.6414 - mean_squared_error: 0.6414\n",
      "Epoch 2/10\n",
      "12037/12037 [==============================] - 45s 4ms/step - loss: 0.3516 - mean_squared_error: 0.3516\n",
      "Epoch 3/10\n",
      "12037/12037 [==============================] - 43s 4ms/step - loss: 0.2980 - mean_squared_error: 0.2980\n",
      "Epoch 4/10\n",
      "12037/12037 [==============================] - 45s 4ms/step - loss: 0.2873 - mean_squared_error: 0.2873\n",
      "Epoch 5/10\n",
      "12037/12037 [==============================] - 45s 4ms/step - loss: 0.3115 - mean_squared_error: 0.3115\n",
      "Epoch 6/10\n",
      "12037/12037 [==============================] - 43s 4ms/step - loss: 0.2869 - mean_squared_error: 0.2869\n",
      "Epoch 7/10\n",
      "12037/12037 [==============================] - 45s 4ms/step - loss: 0.2748 - mean_squared_error: 0.2748\n",
      "Epoch 8/10\n",
      "12037/12037 [==============================] - 45s 4ms/step - loss: 0.2890 - mean_squared_error: 0.2890\n",
      "Epoch 9/10\n",
      "12037/12037 [==============================] - 44s 4ms/step - loss: 0.2857 - mean_squared_error: 0.2857\n",
      "Epoch 10/10\n",
      "12037/12037 [==============================] - 42s 3ms/step - loss: 0.3082 - mean_squared_error: 0.3082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f25c7fcc0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel22 = Sequential()\n",
    "nnmodel22.add(Dense(50, input_dim = 40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel22.add(Dropout(0.4))\n",
    "nnmodel22.add(Dense(40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel22.add(Dropout(0.4))\n",
    "nnmodel22.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "nnmodel22.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "nnmodel22.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 64, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.24547522477168535\n"
     ]
    }
   ],
   "source": [
    "y_test_nn22 = nnmodel22.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_nn22.reshape(5929,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on choosing batch size. <br>\n",
    "Typically, choosing a larger batch size would lead to less accurate predictions. But choosing a very small batch size would affect the training speed of the neural network. Here I stick with a batch size of 32. Through the two practices above, one may observe that increasing the batch size does not significantly improve the MSE. So I will stick with the batch size 32 in the model construction below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel3 = Sequential()\n",
    "nnmodel3.add(Dense(50, input_dim = 40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel3.add(Dropout(0.4))\n",
    "nnmodel3.add(Dense(40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel3.add(Dropout(0.4))\n",
    "nnmodel3.add(Dense(30, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel3.add(Dropout(0.4))\n",
    "nnmodel3.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "nnmodel3.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12037/12037 [==============================] - 153s 13ms/step - loss: 0.6139 - mean_squared_error: 0.6139\n",
      "Epoch 2/10\n",
      "12037/12037 [==============================] - 151s 13ms/step - loss: 0.3733 - mean_squared_error: 0.3733\n",
      "Epoch 3/10\n",
      "12037/12037 [==============================] - 172s 14ms/step - loss: 0.3839 - mean_squared_error: 0.3839\n",
      "Epoch 4/10\n",
      "12037/12037 [==============================] - 167s 14ms/step - loss: 0.3731 - mean_squared_error: 0.3731\n",
      "Epoch 5/10\n",
      "12037/12037 [==============================] - 170s 14ms/step - loss: 0.3297 - mean_squared_error: 0.3297\n",
      "Epoch 6/10\n",
      "12037/12037 [==============================] - 150s 12ms/step - loss: 0.3336 - mean_squared_error: 0.3336\n",
      "Epoch 7/10\n",
      "12037/12037 [==============================] - 147s 12ms/step - loss: 0.3327 - mean_squared_error: 0.3327\n",
      "Epoch 8/10\n",
      "12037/12037 [==============================] - 149s 12ms/step - loss: 0.3283 - mean_squared_error: 0.3283\n",
      "Epoch 9/10\n",
      "12037/12037 [==============================] - 153s 13ms/step - loss: 0.3480 - mean_squared_error: 0.3480\n",
      "Epoch 10/10\n",
      "12037/12037 [==============================] - 149s 12ms/step - loss: 0.3237 - mean_squared_error: 0.3237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f27cabb00>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel3.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.27516862986859597\n"
     ]
    }
   ],
   "source": [
    "y_test_nn3 = nnmodel3.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_nn3.reshape(5929,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel4 = Sequential()\n",
    "nnmodel4.add(Dense(50, input_dim = 40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel4.add(Dropout(0.4))\n",
    "nnmodel4.add(Dense(40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel4.add(Dropout(0.4))\n",
    "nnmodel4.add(Dense(30, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel4.add(Dropout(0.4))\n",
    "nnmodel4.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel4.add(Dropout(0.4))\n",
    "nnmodel4.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "nnmodel4.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12037/12037 [==============================] - 171s 14ms/step - loss: 0.6460 - mean_squared_error: 0.6460\n",
      "Epoch 2/10\n",
      "12037/12037 [==============================] - 171s 14ms/step - loss: 0.4829 - mean_squared_error: 0.4829\n",
      "Epoch 3/10\n",
      "12037/12037 [==============================] - 174s 14ms/step - loss: 0.4575 - mean_squared_error: 0.4575\n",
      "Epoch 4/10\n",
      "12037/12037 [==============================] - 169s 14ms/step - loss: 0.4202 - mean_squared_error: 0.4202\n",
      "Epoch 5/10\n",
      "12037/12037 [==============================] - 167s 14ms/step - loss: 0.3944 - mean_squared_error: 0.3944\n",
      "Epoch 6/10\n",
      "12037/12037 [==============================] - 166s 14ms/step - loss: 0.3873 - mean_squared_error: 0.3873\n",
      "Epoch 7/10\n",
      "12037/12037 [==============================] - 165s 14ms/step - loss: 0.4107 - mean_squared_error: 0.4107\n",
      "Epoch 8/10\n",
      "12037/12037 [==============================] - 167s 14ms/step - loss: 0.3447 - mean_squared_error: 0.3447\n",
      "Epoch 9/10\n",
      "12037/12037 [==============================] - 169s 14ms/step - loss: 0.4002 - mean_squared_error: 0.4002\n",
      "Epoch 10/10\n",
      "12037/12037 [==============================] - 174s 14ms/step - loss: 0.3701 - mean_squared_error: 0.3701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f275f02e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel4.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.24008253771262641\n"
     ]
    }
   ],
   "source": [
    "y_test_nn4 = nnmodel4.predict(x_test_std)\n",
    "print('MSE:', mean_squared_error(y_test_std, y_test_nn4.reshape(5929,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel5 = Sequential()\n",
    "nnmodel5.add(Dense(50, input_dim = 40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel5.add(Dropout(0.4))\n",
    "nnmodel5.add(Dense(40, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel5.add(Dropout(0.4))\n",
    "nnmodel5.add(Dense(30, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel5.add(Dropout(0.4))\n",
    "nnmodel5.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel5.add(Dropout(0.4))\n",
    "nnmodel5.add(Dense(10, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nnmodel5.add(Dropout(0.4))\n",
    "nnmodel5.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "nnmodel5.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12037/12037 [==============================] - 218s 18ms/step - loss: 0.7347 - mean_squared_error: 0.7347\n",
      "Epoch 2/10\n",
      "12037/12037 [==============================] - 213s 18ms/step - loss: 0.6092 - mean_squared_error: 0.6092\n",
      "Epoch 3/10\n",
      "12037/12037 [==============================] - 216s 18ms/step - loss: 0.5030 - mean_squared_error: 0.5030\n",
      "Epoch 4/10\n",
      "12037/12037 [==============================] - 223s 19ms/step - loss: 0.4815 - mean_squared_error: 0.4815\n",
      "Epoch 5/10\n",
      "12037/12037 [==============================] - 228s 19ms/step - loss: 0.4716 - mean_squared_error: 0.4716\n",
      "Epoch 6/10\n",
      "12037/12037 [==============================] - 219s 18ms/step - loss: 0.4954 - mean_squared_error: 0.4954\n",
      "Epoch 7/10\n",
      "12037/12037 [==============================] - 213s 18ms/step - loss: 0.4941 - mean_squared_error: 0.4941\n",
      "Epoch 8/10\n",
      "12037/12037 [==============================] - 210s 17ms/step - loss: 0.4571 - mean_squared_error: 0.4571\n",
      "Epoch 9/10\n",
      "12037/12037 [==============================] - 212s 18ms/step - loss: 0.3932 - mean_squared_error: 0.3932\n",
      "Epoch 10/10\n",
      "12037/12037 [==============================] - 221s 18ms/step - loss: 0.4588 - mean_squared_error: 0.4588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f26df26d8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel5.fit(x_tr_std, y_tr_std, epochs = 10, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.30460518887444227\n"
     ]
    }
   ],
   "source": [
    "y_test_nn5 = nnmodel5.predict(x_test_std)\n",
    "print('MSE', mean_squared_error(y_test_std, y_test_nn5.reshape(5929,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed above, having more layer is not equivalent to better prediction. The model with the best MSE score is neural network model 2. It has the following layers\n",
    "\n",
    "| Layer | # of Inputs | # of Outputs | Activation Function | Dropout Rate |\n",
    "|------|------|------|------|------|\n",
    "| 1 | 40 | 50 | Relu | 0.4 |\n",
    "| 2 | 50 | 40 | Relu | 0.4 | \n",
    "| 3 | 40 | 1 | Linear| N/A |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Model Evaluation\n",
    "Model evaluation is based on the score, specifically mean squared error (MSE) on the test set. Based on the model trained, how does the model perform on an unseen dataset. The mean squared error is calculated by the mean of the square of errors. It is an effective way to measure how different is the prediction from the actual value. The equation of MSE is given by: <br>\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2$ <br>\n",
    "The following table compares the performance of different model algorithms: \n",
    "\n",
    "| Model | MSE Score | Hyperparameter/Layers | Interpretable | \n",
    "|------|------|------|------| \n",
    "| Linear regression with LASSO regularization | $\\approx 0.256$ | $\\alpha = 10^{-20}$ | Coefficients can be used to interpret feature importance |\n",
    "| Random Forest | $\\approx 0.203$ | # of trees in forest = 500, maximum depth of tree = 20, maximum feature = 41 | Low |\n",
    "| Neural Network | $\\approx 0.239$ | 4 layers in total (Neurons 41>50>40>30>1) | Low |\n",
    "\n",
    "The model that performs the best prediction on the unseen dataset is the random forest model. Using this model, one can predict a player's wage given the player's profile with the least MSE. Sometimes there might be missing data in the player's profile. The idea of how to deal with missing data is described in the data preprocessing section. Then one can easily predict the player's wage with little error. For the club owners and managers, they can decide on the wages of the player based on this player's attributes. This is a reliable way to negotiate with player without underpaying or overpaying. <br>\n",
    "However, neural network model does not provide a understanding on how to interpret FIFA players' wages. It would be good to know intuitively which feature(s) of the players affect their wages more than others. This analysis of feature importance will be discussed in the next section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Model Deployment\n",
    "In the previous section, I have discussed which model to use in predicting the players' wages. Now let's turn to an intuitive side\n",
    "\n",
    "| Feature | Description | Coefficient | Importance |\n",
    "|------|------|------|------|\n",
    "| **Age** | Age | $\\approx 0.0884$ | important, positive |\n",
    "| Overall | Overall rating | $\\approx -0.0144$ | little importance, negative | \n",
    "| Potential | Potential rating | $\\approx 0.0321$ | little importance, positive |\n",
    "| **Value** | Current Market Value | $\\approx 0.844$ | high importance, positive |\n",
    "| Height | Height | $\\approx 0.0254$ | little importance, positive | \n",
    "| Crossing | Rating of crossing | $\\approx 0.0338$ | little importance, positive |\n",
    "| Finishing | Rating of finishing | $\\approx -0.0168$ | little importance, negative |\n",
    "| Dribbling | Rating of dribbling | $\\approx 0.0162$ | little importance, positive |\n",
    "| Curve | Rating of curve | $\\approx 0.0175$ | little importance, positive |\n",
    "| FKAccuracy | Rating of free kick accuracy | $\\approx -0.0554$ | somewhat important, negative |\n",
    "| LongPassing| Rating of long passing | $\\approx -0.0208$ | little importance, negative |\n",
    "| Acceleration | Rating of acceleration | $\\approx -0.0161$ | little importance, negative |\n",
    "| Balance | Rating of balance | $\\approx 0.0225$ | little importance, positive |\n",
    "| ShotPower | Rating of shot power | $\\approx 0.0188$ | little importance, positive |\n",
    "| Stamina | Rating of stamina | $\\approx -0.0547$ | somewhat important, negative | \n",
    "| Strength | Rating of strength | $\\approx -0.0185$ | little importance, negative |\n",
    "| LongShots | Rating of long shots | $\\approx 0.0139$ | little importance, positive | \n",
    "| Penalties | Rating of penalties | $\\approx 0.0368$ | little importance, positive |\n",
    "| **SlidingTackle** | Rating of sliding tackle | $\\approx 0.0658$ | somewhat important, positive |\n",
    "| GKHandling | Rating of goal keeper handling | $\\approx 0.0141$ | little importance, positive |\n",
    "\n",
    "As observed above, the top three features affecting the wage of the FIFA player are: age of the player, current market value of the player and the rating of sliding tackle (marked in bold text). When evaluating the wage of player, one may use these three features together, rather than using one feature, to estimate the wage of a player. For club seeking recruitment, they can use these three features to estimate the wages of players and check whether they would be a good match. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Insights\n",
    "For club owners and managers, if they're looking for a player to recruit, they may search for a player that matches them the best. While players with better performance and ability are always preferred, clubs cannot afford to recruit a player with a wage beyond their budget. So this project is useful for the soccer club owners in two ways: <br>\n",
    "\n",
    "----\n",
    "1. During the early recruitment stage, use the analysis on the feature importance in the previous section to briefly estimate the wage of the all players and seek for the most appropriate candidates in terms of both affordability and performance\n",
    "2. Once appropriate candidate is found, the club may use the random forest model to work out the best wage prediction to gain an advantage during the negotiations. \n",
    "\n",
    "-----\n",
    "\n",
    "With that being said, predicting the wages of FIFA players is very beneficial for the club owners and managers to maximize their benefit. <br>\n",
    "<br>\n",
    "There's also an insight for future research. Soccer players have different positions, and different positions may value different attributes. Further study could divide the players' data into categories of positions to achieve better understanding of players' wage. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
